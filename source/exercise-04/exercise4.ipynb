{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "#   Exercise4 notebook.\n",
    "#\n",
    "# Copyright (C) 2018 Santiago Cortes, Juha Ylioinas\n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later); please refer to the file \n",
    "# Licence.txt, included with the software, for details.\n",
    "\n",
    "# Preparations\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from itertools import compress\n",
    "\n",
    "from scipy.ndimage import maximum_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import convolve1d as conv1\n",
    "from scipy.ndimage.filters import convolve as conv2\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import ProjectiveTransform, SimilarityTransform, AffineTransform\n",
    "from skimage.measure import ransac\n",
    "\n",
    "from utils import gaussian2, maxinterp, circle_points\n",
    "\n",
    "import time\n",
    "\n",
    "# Select data directory\n",
    "\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-04-data/')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 4\n",
    "The problems should be solved before the exercise session and solutions returned via\n",
    "MyCourses. Upload to MyCourses both: this Jupyter Notebook (.ipynb) file containing your solutions to the programming tasks and the exported pdf version of this Notebook file. If there are both programming and pen & paper tasks kindly combine the two pdf files (your scanned/LaTeX solutions and the exported Notebook) into a single pdf and submit that with the Notebook (.ipynb) file. <br><br> Note that (1) you are not supposed to change anything in the utils.py and (2) you should be sure that everything that you need to implement should work with the pictures specified by the assignments of this exercise round. \n",
    "\n",
    "<b>NOTE: In order to avoid errors caused by running the cells in mixed order (which quite often happens while trying different things and debugging), while working on a particular cell be sure that you have freshly run all its preceding cells belonging to the same exercise. <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your name and student number below.\n",
    "\n",
    "### Name:\n",
    "### Student number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Matching Harris corner points\n",
    "In this exercise, you will familiarize yourself with the method of Harris interest point detection. The aim is to first detect Harris corners from two images of the same scene. Then, image patches of size 15x15 pixels around each detected corner point is extracted following a matching step where mutually nearest neighbors are found using the sum of squared differences (SSD) similarity measure. <br><br>\n",
    "The SSD measure for two image patches, $f$ and $g$, is defined as follows<br><br>\n",
    "\n",
    "<center>$SSD(f,g) = \\sum_{k,l}(g(k,l)-f(k,l))^{2}$</center> \n",
    "so that the larger the SSD value the more dissimilar the patches are.\n",
    "<br><br>\n",
    "Do the task (a) below and answer questions in (b):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The first part uses OpenCV computer vision library to\n",
    "## extract Harris corner points\n",
    "## (source: https://docs.opencv.org/3.0-beta/doc/py_tutorials/\n",
    "## py_feature2d/py_features_harris/py_features_harris.html)\n",
    "I1 = imread(data_dir+'Boston1.png');\n",
    "R1 = cv2.cornerHarris(I1,2,3,0.04)\n",
    "\n",
    "# Take only the local maxima of the corner response function\n",
    "fp = np.ones((3,3))\n",
    "fp[1,1] = 0\n",
    "maxNR1 = maximum_filter(R1, footprint=fp, mode='constant')\n",
    "\n",
    "# Test if cornerness is larger than neighborhood\n",
    "cornerI1 = R1>maxNR1\n",
    "\n",
    "# Threshold for low value maxima\n",
    "maxCV1 = np.amax(R1)\n",
    "\n",
    "# Find centroids\n",
    "ret, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8((R1>0.0001*maxCV1)*cornerI1))\n",
    "\n",
    "# Define the criteria to stop and refine the corners\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "corners = cv2.cornerSubPix(I1,np.float32(centroids),(5,5),(-1,-1), criteria)\n",
    "kp1=corners.T\n",
    "\n",
    "# Display Harris keypoints\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(I1, cmap='gray')\n",
    "plt.plot([kp1[0]],[kp1[1]],'rx')\n",
    "plt.suptitle(\"Harris Corners using OpenCV\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The previous part illustrated OpenCV's built-in capabilities.\n",
    "## Let's try to do Harris corner extraction and matching using our own\n",
    "## implementation in a less black-box manner.\n",
    "\n",
    "## Familiarize yourself with the harris function\n",
    "def harris(im, sigma=1.0, relTh=0.0001, k=0.04):\n",
    "    im = im.astype(np.float) # Make sure im is float\n",
    "    \n",
    "    # Get smoothing and derivative filters\n",
    "    g, _, _, _, _, _, = gaussian2(sigma)\n",
    "    _, gx, gy, _, _, _, = gaussian2(np.sqrt(0.5))\n",
    "    \n",
    "    # Partial derivatives\n",
    "    Ix = conv2(im, -gx, mode='constant')\n",
    "    Iy = conv2(im, -gy, mode='constant')\n",
    "    \n",
    "    # Components of the second moment matrix\n",
    "    Ix2Sm = conv2(Ix**2, g, mode='constant')\n",
    "    Iy2Sm = conv2(Iy**2, g, mode='constant')\n",
    "    IxIySm = conv2(Ix*Iy, g, mode='constant')\n",
    "    \n",
    "    # Determinant and trace for calculating the corner response\n",
    "    detC = (Ix2Sm*Iy2Sm)-(IxIySm**2)\n",
    "    traceC = Ix2Sm+Iy2Sm\n",
    "    \n",
    "    # Corner response function R\n",
    "    # \"Corner\": R > 0\n",
    "    # \"Edge\": R < 0\n",
    "    # \"Flat\": |R| = small\n",
    "    R = detC-k*traceC**2\n",
    "    maxCornerValue = np.amax(R)\n",
    "    \n",
    "    # Take only the local maxima of the corner response function\n",
    "    fp = np.ones((3,3))\n",
    "    fp[1,1] = 0\n",
    "    maxImg = maximum_filter(R, footprint=fp, mode='constant')\n",
    "    \n",
    "    # Test if cornerness is larger than neighborhood\n",
    "    cornerImg = R>maxImg\n",
    "    \n",
    "    # Threshold for low value maxima\n",
    "    y, x = np.nonzero((R>relTh*maxCornerValue)*cornerImg) \n",
    "    \n",
    "    # Convert to float\n",
    "    x = x.astype(np.float)\n",
    "    y = y.astype(np.float)\n",
    "    \n",
    "    # Remove responses from image borders to reduce false corner detections\n",
    "    r, c = R.shape\n",
    "    idx = np.nonzero((x<2)+(x>c-3)+(y<2)+(y>r-3))[0]\n",
    "    x = np.delete(x,idx)\n",
    "    y = np.delete(y,idx)\n",
    "    \n",
    "    # Parabolic interpolation\n",
    "    for i in range(len(x)):\n",
    "        _,dx=maxinterp((R[int(y[i]), int(x[i])-1], R[int(y[i]), int(x[i])], R[int(y[i]), int(x[i])+1]))\n",
    "        _,dy=maxinterp((R[int(y[i])-1, int(x[i])], R[int(y[i]), int(x[i])], R[int(y[i])+1, int(x[i])]))\n",
    "        x[i]=x[i]+dx\n",
    "        y[i]=y[i]+dy\n",
    "        \n",
    "    return x, y, cornerImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "I1 = imread(data_dir+'Boston1.png')/255.\n",
    "I2 = imread(data_dir+'Boston2m.png')/255.\n",
    "\n",
    "# Harris corner extraction, take a look at the source code above\n",
    "x1, y1, cimg1 = harris(I1)\n",
    "x2, y2, cimg2 = harris(I2)\n",
    "\n",
    "## We pre-allocate the memory for the 15*15 image patches extracted\n",
    "## around each corner point from both images\n",
    "patch_size=15\n",
    "npts1=x1.shape[0]\n",
    "npts2=x2.shape[0]\n",
    "patches1=np.zeros((patch_size, patch_size, npts1))\n",
    "patches2=np.zeros((patch_size, patch_size, npts2))\n",
    "\n",
    "## The following part extracts the patches using bilinear interpolation\n",
    "k=(patch_size-1)/2.\n",
    "xv,yv=np.meshgrid(np.arange(-k,k+1),np.arange(-k, k+1))\n",
    "for i in range(npts1):\n",
    "    patch = map_coordinates(I1, (yv + y1[i], xv + x1[i]))\n",
    "    patches1[:,:,i] = patch\n",
    "for i in range(npts2):\n",
    "    patch = map_coordinates(I2, (yv + y2[i], xv + x2[i]))\n",
    "    patches2[:,:,i] = patch\n",
    "\n",
    "## We compute the sum of squared differences (SSD) of pixels' intensities\n",
    "## for all pairs of patches extracted from the two images\n",
    "distmat = np.zeros((npts1, npts2))\n",
    "for i1 in range(npts1):\n",
    "    for i2 in range(npts2):\n",
    "        distmat[i1,i2]=np.sum((patches1[:,:,i1]-patches2[:,:,i2])**2)\n",
    "\n",
    "## Next we compute pairs of patches that are mutually nearest neighbors\n",
    "## according to the SSD measure\n",
    "ss1 = np.amin(distmat, axis=1)\n",
    "ids1 = np.argmin(distmat, axis=1)\n",
    "ss2 = np.amin(distmat, axis=0)\n",
    "ids2 = np.argmin(distmat, axis=0)\n",
    "\n",
    "pairs = []\n",
    "for k in range(npts1):\n",
    "    if k == ids2[ids1[k]]:\n",
    "        pairs.append(np.array([k, ids1[k], ss1[k]]))\n",
    "pairs = np.array(pairs)\n",
    "\n",
    "## We sort the mutually nearest neighbors based on the SSD\n",
    "sorted_ssd = np.sort(pairs[:,2], axis=0)\n",
    "id_ssd = np.argsort(pairs[:,2], axis=0)\n",
    "\n",
    "## Estimate the geometric transformation between images\n",
    "src=[]\n",
    "dst=[]\n",
    "for k in range(len(id_ssd)):\n",
    "    l = id_ssd[k]\n",
    "    src.append([x1[int(pairs[l, 0])], y1[int(pairs[l, 0])]])\n",
    "    dst.append([x2[int(pairs[l, 1])], y2[int(pairs[l, 1])]])\n",
    "src=np.array(src)\n",
    "dst=np.array(dst)\n",
    "rthrs=2\n",
    "tform,_ = ransac((src, dst), ProjectiveTransform, min_samples=4,\n",
    "                               residual_threshold=rthrs, max_trials=1000)\n",
    "H1to2p = tform.params\n",
    "\n",
    "## Next we visualize the 40 best matches which are mutual nearest neighbors\n",
    "## and have the smallest SSD values\n",
    "Nvis = 40\n",
    "montage = np.concatenate((I1, I2), axis=1)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"The best 40 matches according to SSD measure\", fontsize=20)\n",
    "plt.imshow(montage, cmap='gray')\n",
    "plt.title('The best 40 matches')\n",
    "for k in range(np.minimum(len(id_ssd), Nvis)):\n",
    "    l = id_ssd[k]\n",
    "    plt.plot(x1[int(pairs[l, 0])], y1[int(pairs[l, 0])], 'rx')\n",
    "    plt.plot(x2[int(pairs[l, 1])] + I1.shape[1], y2[int(pairs[l, 1])], 'rx')\n",
    "    plt.plot([x1[int(pairs[l, 0])], x2[int(pairs[l, 1])]+I1.shape[1]], \n",
    "         [y1[int(pairs[l, 0])], y2[int(pairs[l, 1])]])\n",
    "\n",
    "## Finally, since we have estimated the planar projective transformation\n",
    "## we can check that how many of the nearest neighbor matches actually\n",
    "## are correct correspondences\n",
    "p1to2=np.dot(H1to2p, np.hstack((src, np.ones((src.shape[0],1)))).T)\n",
    "p1to2 = p1to2[:2,:] / p1to2[2,:]\n",
    "p1to2 = p1to2.T\n",
    "pdiff=np.sqrt(np.sum((dst-p1to2)**2, axis=1))\n",
    "\n",
    "# The criterion for the match being a correct is that its correspondence in\n",
    "# the second image should be at most rthrs=2 pixels away from the transformed\n",
    "# location\n",
    "n_correct = len(pdiff[pdiff<rthrs])\n",
    "print(\"{} correct matches.\".format(n_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Matching points using normalized cross-correlation (NCC)\n",
    "Implement the matching of mutually nearest neighbors using normalized cross-correlation\n",
    "(NCC) as the similarity measure instead of SSD. <br><br>\n",
    "\n",
    "For two image patches of similar size it can be written as follows (also given in the slide 97 of\n",
    "Lecture 2):\n",
    "<br><br>\n",
    "<center>$NCC(f,g) = \\frac{\\sum_{k,l}(g(k,l)-\\bar{g})(f(k,l)-\\bar{f})}{\\sqrt{\\sum_{k,l}(g(k,l)-\\bar{g})^{2}\\sum_{k,l}(f(k,l)-\\bar{f})^{2}}}$</center> <br><br>\n",
    "where $\\bar{g}$ and $\\bar{f}$ are the mean intensity values of patches $g$ and $f$. The values of NCC are\n",
    "always between -1 and 1, and the larger the value the more similar the patches are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, your task is to do matching in similar manner but using normalised \n",
    "## cross-correlation (NCC) instead of SSD. You should also report the \n",
    "## number of correct correspondences for NCC as shown above for SSD.\n",
    "##\n",
    "## HINT: Compared to the previous SDD-based implementation, all you need \n",
    "## to do is to modify the lines performing the 'distmat' calculation\n",
    "## from SSD to NCC.\n",
    "## Thereafter, you can proceed as above but notice the following details:\n",
    "## You need to determine the mutually nearest neighbors by\n",
    "## finding pairs for which NCC is maximized (i.e. not minimized like SSD).\n",
    "## Also, you need to sort the matches in descending order in terms of NCC\n",
    "## in order to find the best matches (i.e. not ascending order as with SSD). \n",
    "\n",
    "# Measure pairwise distances NCC\n",
    "##-your-code-starts-here-##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##-your-code-ends-here-##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Answer the questions below\n",
    "1) How  many  correct  correspondences  do  you  get  by  using  NCC  instead  of  SSD?\n",
    "<br>\n",
    "2) Which  one  of  the  two  similarity  measures  performs  better  in  this  case  and  why? <br>\n",
    "\n",
    "Type your answers here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Matching SURF regions\n",
    "SURF (Speeded up robust features) is quite similar to SIFT which was presented in Lecture 3. In this implementation the descriptor vectors for the local regions have 64 elements (instead of 128 in SIFT) but\n",
    "Euclidean distance can still be used as a similarity measure in descriptor space. See the\n",
    "comments in the source code and do the following tasks:<br><br>\n",
    "a) Sort the given nearest neighbor matches in ascending order based on the nearest\n",
    "neighbor distance ratio (NNDR), which is defined in Equation (4.18) in the course book.\n",
    "Report the number of correct correspondences among the top 5 matches based on NNDR\n",
    "and compare it to the case where ordering is based on nearest neighbor distance.<br>\n",
    "b) Answer some questions (see them below... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The first part uses OpenCV computer vision and scikit's image processing\n",
    "## libraries.\n",
    "## SURF regions are extracted and matched and a similarity transformation \n",
    "## (i.e. rotation, translation and scale) between the views is estimated\n",
    "img1 = np.array(Image.open(data_dir+'boat1.png'))\n",
    "img2 = np.array(Image.open(data_dir+'boat6.png'))\n",
    "\n",
    "# Initiate SURF detector\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "# Find the keypoints and descriptors with SIFT detector\n",
    "kp1, desc1 = surf.detectAndCompute(img1, None)\n",
    "kp2, desc2 = surf.detectAndCompute(img2, None)\n",
    "kps1 = np.array([p.pt for p in kp1])\n",
    "kps2 = np.array([p.pt for p in kp2])\n",
    "kps1_rad = np.array([p.size / 2 for p in kp1]) #rad==scale\n",
    "kps2_rad = np.array([p.size / 2 for p in kp2])\n",
    "\n",
    "## Shift should work this year (2019)-> Code below should not be needed. --------------------------------\n",
    "## You may use the lines below if you do not have opencv compiled with opencv-contrib \n",
    "## (surf and sift are only part of that as they are patented)\n",
    "\n",
    "## Precomputed features and descriptors\n",
    "\n",
    "## Using a trick to circumvent a bug in the new version of np.load\n",
    "## save np.load\n",
    "#np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "#data1=np.load(data_dir+\"img1_surf_kps_descs.npy\", encoding='latin1')\n",
    "#data2=np.load(data_dir+\"img2_surf_kps_descs.npy\", encoding='latin1')\n",
    "\n",
    "## restore np.load for future normal usage\n",
    "#np.load = np_load_old\n",
    "\n",
    "#kps1 = data1.item().get('keypoints')\n",
    "#kps1_rad = data1.item().get('keypoint_rads')\n",
    "#desc1 = data1.item().get('descriptors')\n",
    "\n",
    "#kps2 = data2.item().get('keypoints')\n",
    "#kps2_rad = data2.item().get('keypoint_rads')\n",
    "#desc2 = data2.item().get('descriptors')\n",
    "\n",
    "#kp1 = []\n",
    "#kp2 = []\n",
    "#for i in range(kps1.shape[0]):\n",
    "#    p=cv2.KeyPoint()\n",
    "#    p.pt = (kps1[i,0], kps1[i,1]) # coordinates of the keaypoints\n",
    "#    p.size = kps1_rad[i] * 2 # diameter of the blob feature\n",
    "#    kp1.append(p)\n",
    "    \n",
    "#for i in range(kps2.shape[0]):\n",
    "#    p=cv2.KeyPoint()\n",
    "#    p.pt = (kps2[i,0], kps2[i,1])\n",
    "#    p.size = kps2_rad[i] * 2\n",
    "#    kp2.append(p)\n",
    "## ------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initiate BruteForce matcher with default params\n",
    "bf = cv2.BFMatcher()\n",
    "# Perform matching and save k=1 nearest neighbors for each descriptor\n",
    "matches = bf.knnMatch(desc1, desc2, k=1)\n",
    "# The candidate point matches can be visualized as follows:\n",
    "img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,flags=2)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.suptitle('Feature matching using SURF regions', fontsize=20)\n",
    "plt.imshow(img3)\n",
    "plt.title('Candidate point matches')\n",
    "plt.show()\n",
    "\n",
    "## The estimation of geometric transformations is covered later in lectures\n",
    "## but it can be done as follows using scikit-image Python library:\n",
    "# Collect feature points and scales from the match objects\n",
    "source_pts = []\n",
    "target_pts = []\n",
    "\n",
    "for match in matches:\n",
    "    # Collect feature point coords and scale query (img1)\n",
    "    x, y = kp1[match[0].queryIdx].pt \n",
    "    source_pts.append(np.array([x, y]))  \n",
    "    # Collect feature point coords and scale query (img2)\n",
    "    x, y = kp2[match[0].trainIdx].pt\n",
    "    target_pts.append(np.array([x, y]))\n",
    "    \n",
    "source_pts = np.array(source_pts)\n",
    "target_pts = np.array(target_pts)\n",
    "\n",
    "## Estimate the geometric transformation between images\n",
    "rthrs=10\n",
    "tform, inliers = ransac((source_pts, target_pts), SimilarityTransform, min_samples=2,\n",
    "                               residual_threshold=rthrs, max_trials=1000)\n",
    "H1to2p = tform.params\n",
    "\n",
    "s_in = source_pts[inliers,:]\n",
    "t_in = target_pts[inliers,:]\n",
    "\n",
    "source_pts_aug = np.hstack((s_in,np.ones((s_in.shape[0],1))))\n",
    "target_pts_aug = np.hstack((t_in,np.ones((t_in.shape[0],1))))\n",
    "\n",
    "target_ = np.dot(H1to2p,source_pts_aug.T)\n",
    "target_ = target_[:2,:] / target_[2,:]\n",
    "target_ = target_.T\n",
    "\n",
    "xv, yv = np.meshgrid(np.arange(0,img1.shape[1]), np.arange(0,img1.shape[0]))\n",
    "src_all = np.vstack((xv.flatten(), yv.flatten(), np.ones((1, xv.size))))\n",
    "target_all = np.dot(H1to2p, src_all)\n",
    "target_all_ = target_all[:2,:] / target_all[2,:]\n",
    "xvt = target_all_[0,:].reshape(xv.shape[0], xv.shape[1])\n",
    "yvt = target_all_[1,:].reshape(yv.shape[0], yv.shape[1])\n",
    "img2t = map_coordinates(img2, (yvt, xvt))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16,8))\n",
    "ax = axes.ravel()\n",
    "ax[0].imshow(img1, cmap='gray')\n",
    "ax[0].set_title(\"Input Image 1\")\n",
    "ax[1].imshow(img2t, cmap='gray')\n",
    "ax[1].set_title(\"Transformed Image 2\")\n",
    "ax[2].imshow(np.abs(img1-img2t), cmap='gray')\n",
    "ax[2].set_title(\"Difference image after geometric registration\")\n",
    "\n",
    "matches_in = list(compress(matches, inliers))\n",
    "img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches_in,None,flags=2)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(img3)\n",
    "plt.title(\"Matched inlier points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The previous part illustrated OpenCV's built-in brute force matcher.\n",
    "## Let's do the nearest neighbor matching for feature vectors in desc1 and desc2\n",
    "## by using our own implementation.\n",
    "\n",
    "## We compute the pairwise distances of feature vectors to matrix 'distmat'\n",
    "## you can use the for-loop version or faster vectorized version\n",
    "#distmat = np.zeros((desc1.shape[0], desc2.shape[0]))\n",
    "#for i in range(desc1.shape[0]):\n",
    "#    for j in range(desc2.shape[0]):\n",
    "#        distmat[i,j] = np.linalg.norm(desc1[i,:] - desc2[j,:])\n",
    "## Vectorized version: sqrt(xTx + yTy - 2xTy)\n",
    "distmat = np.dot(desc1, desc2.T)\n",
    "X_terms = np.expand_dims(np.diag(np.dot(desc1, desc1.T)), axis=1)\n",
    "X_terms = np.tile(X_terms,(1,desc2.shape[0]))\n",
    "Y_terms = np.expand_dims(np.diag(np.dot(desc2, desc2.T)), axis=0)\n",
    "Y_terms = np.tile(Y_terms,(desc1.shape[0],1))\n",
    "distmat = np.sqrt(Y_terms + X_terms - 2*distmat)\n",
    "\n",
    "## We determine the mutually nearest neighbors\n",
    "dist1 = np.amin(distmat, axis=1)\n",
    "ids1 = np.argmin(distmat, axis=1)\n",
    "dist2 = np.amin(distmat, axis=0)\n",
    "ids2 = np.argmin(distmat, axis=0)\n",
    "\n",
    "pairs = []\n",
    "for k in range(ids1.size):\n",
    "    if k == ids2[ids1[k]]:\n",
    "        pairs.append(np.array([k, ids1[k], dist1[k]]))\n",
    "pairs = np.array(pairs)\n",
    "\n",
    "# We sort the mutually nearest neighbors based on the distance \n",
    "snnd = np.sort(pairs[:,2], axis=0)\n",
    "id_nnd = np.argsort(pairs[:,2], axis=0)\n",
    "\n",
    "# We visualize the 5 best matches \n",
    "Nvis = 5\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Top 5 mutual nearest neigbors of SURF features\", fontsize=20)\n",
    "plt.imshow(np.hstack((img1, img2)), cmap='gray')\n",
    "\n",
    "t = np.arange(0, 2*np.pi, 0.1)\n",
    "\n",
    "# Display matches\n",
    "for k in range(Nvis):\n",
    "    pid1 = pairs[id_nnd[k], 0]\n",
    "    pid2 = pairs[id_nnd[k], 1]\n",
    "    \n",
    "    loc1 = kps1[int(pid1)]\n",
    "    r1 = 6*kps1_rad[int(pid1)]\n",
    "    loc2 = kps2[int(pid2)]\n",
    "    r2 = 6*kps2_rad[int(pid2)]\n",
    "    \n",
    "    plt.plot(loc1[0]+r1*np.cos(t), loc1[1]+r1*np.sin(t), 'm-', linewidth=3)\n",
    "    plt.plot(loc2[0]+r2*np.cos(t)+img1.shape[1], loc2[1]+r2*np.sin(t), 'm-', linewidth=3)\n",
    "    plt.plot([loc1[0], loc2[0]+img1.shape[1]], [loc1[1], loc2[1]], 'c-')\n",
    "    \n",
    "# How many of the top 5 matches appear to be correct correspondences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Sorting matches according to the nearest neighbor distance ratio (NNDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, your task is to compute and visualize the top 5 matches based on \n",
    "## the nearest neighbor distance ratio defined in Equation (4.18) in the course book.\n",
    "## How many of those are correct correspondences?\n",
    "\n",
    "##-your-code-starts-here-##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##-your-code-ends-here-##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Answer the questions below\n",
    "1) What are the benefits of using SURF regions instead of Harris corners? \n",
    "<br>\n",
    "2) Why the matching approach of Exercise 1 (i.e. Harris corners and NCC based matching) would not work for the example images of Exercise 2? \n",
    "<br>\n",
    "3) In what kind of cases Harris corners may still be better than SURF and why?\n",
    "\n",
    "Type your answers here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Scale-space blob detection\n",
    "\n",
    "The python lines below illustrate pre-computed blob detections obtained with a similar procedure as implemented in SIFT and described below. Here the task is to replace the pre-computed regions with regions computed\n",
    "by your own implementation. The result does not need to be exactly the same as the\n",
    "pre-computed one but similar. In summary, implement the scale-space blob detector as\n",
    "follows:\n",
    "<br>a) Generate a Laplacian of Gaussian filter (you can set Ïƒ = 0.5).\n",
    "<br>b) Build a Laplacian scale space, starting with some initial scale and going for n iterations:\n",
    "- filter image with scale-normalized Laplacian at current scale\n",
    "- save square of Laplacian response for current level of scale space\n",
    "- increase scale by factor k\n",
    "\n",
    "c) Perform non-maximum suppression in scale space.\n",
    "<br>d) Display resulting circles at their characteristic scales.\n",
    "<br><br>Apply the blob detector to example images boat1.png and boat6.png as shown in the\n",
    "example script. Can you identify some corresponding regions?<br><br>\n",
    "Note 1: Suitable values for k and n could be k = 1.19 and n = 18.<br>\n",
    "Note 2: This task corresponds to Exercise 4.1 in the course book. A similar assignment\n",
    "has been used by Lazebnik at UIUC and their course page gives also more detailed\n",
    "instructions: http://slazebni.cs.illinois.edu/spring16/assignment2.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "img1 = np.array(Image.open(data_dir+'boat1.png'))\n",
    "img2 = np.array(Image.open(data_dir+'boat6.png'))\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# Find the keypoints and descriptors with SIFT detector\n",
    "kp1, desc1 = sift.detectAndCompute(img1, None)\n",
    "kp2, desc2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "## Sift should work this year (2019). -> Code below should not be needed. -----------------------------------\n",
    "## The same song here as in the previous exercise, no sift and surf if you dont compile\n",
    "## with opencv-contrib, sorry. :L\n",
    "\n",
    "## Using the same trick to circumvent a bug in the new version of np.load\n",
    "## save np.load\n",
    "#np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "#data1=np.load(data_dir+\"boat1_sift_kps_descs.npy\", encoding='latin1')\n",
    "#data2=np.load(data_dir+\"boat6_sift_kps_descs.npy\", encoding='latin1')\n",
    "\n",
    "## restore np.load for future normal usage\n",
    "#np.load = np_load_old\n",
    "\n",
    "#kps1 = data1.item().get('keypoints')\n",
    "#kps1_rad = data1.item().get('keypoint_rads')\n",
    "#desc1 = data1.item().get('descriptors')\n",
    "\n",
    "#kps2 = data2.item().get('keypoints')\n",
    "#kps2_rad = data2.item().get('keypoint_rads')\n",
    "#desc2 = data2.item().get('descriptors')\n",
    "\n",
    "#kp1 = []\n",
    "#kp2 = []\n",
    "#for i in range(kps1.shape[0]):\n",
    "#    p=cv2.KeyPoint()\n",
    "#    p.pt = (kps1[i,0], kps1[i,1])\n",
    "#    p.size = kps1_rad[i] * 2\n",
    "#    kp1.append(p)\n",
    "    \n",
    "#for i in range(kps2.shape[0]):\n",
    "#    p=cv2.KeyPoint()\n",
    "#    p.pt = (kps2[i,0], kps2[i,1])\n",
    "#    p.size = kps2_rad[i] * 2\n",
    "#    kp2.append(p)\n",
    "## ------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initiate BruteForce matcher with default params\n",
    "bf = cv2.BFMatcher()\n",
    "# Perform matching and save k=2 nearest neighbors for each descriptor\n",
    "matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "# Apply Lowe's ratio test\n",
    "good_matches = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good_matches.append(m)\n",
    "# Sort matches \n",
    "good_matches = sorted(good_matches, key = lambda x:x.distance)\n",
    "# Collect feature points and scales from the match objects\n",
    "source_pts = []\n",
    "target_pts = []\n",
    "source_radii = []\n",
    "target_radii = []\n",
    "\n",
    "for match in good_matches:\n",
    "    # Collect feature point coords and scale query (img1)\n",
    "    x, y = kp1[match.queryIdx].pt\n",
    "    pt = np.array([np.round(x), np.round(y)]).astype(np.int)\n",
    "    source_pts.append(pt)\n",
    "    radius = kp1[match.queryIdx].size / 2.\n",
    "    source_radii.append(radius)\n",
    "    \n",
    "    # Collect feature point coords and scale query (img2)\n",
    "    x, y = kp2[match.trainIdx].pt\n",
    "    pt = np.array([np.round(x), np.round(y)]).astype(np.int)\n",
    "    target_pts.append(pt)\n",
    "    radius = kp2[match.trainIdx].size / 2.\n",
    "    target_radii.append(radius)\n",
    "    \n",
    "source_pts = np.array(source_pts)\n",
    "source_radii = np.array(source_radii)\n",
    "target_pts = np.array(target_pts)\n",
    "target_radii = np.array(target_radii)\n",
    "\n",
    "## Estimate the geometric transformation between images\n",
    "rthrs=10\n",
    "tform,_ = ransac((source_pts, target_pts), SimilarityTransform, min_samples=2,\n",
    "                               residual_threshold=rthrs, max_trials=1000)\n",
    "H1to2p = tform.params\n",
    "s = np.sqrt(np.linalg.det(H1to2p[0:2,0:2]));\n",
    "R = H1to2p[0:2,0:2] / s;\n",
    "t = H1to2p[0:2,2];\n",
    "\n",
    "# Plot \n",
    "montage = np.concatenate((img1, img2), axis=1)\n",
    "Nvis = 20\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.suptitle(\"Matching points using SIFT\", fontsize=20)\n",
    "plt.imshow(montage, cmap='gray')\n",
    "plt.title('The best {} matches'.format(Nvis))\n",
    "for k in range(0, Nvis):   \n",
    "    plt.plot([source_pts[k,0], target_pts[k,0]+img1.shape[1]],\\\n",
    "             [source_pts[k,1], target_pts[k,1]], 'r-')\n",
    "    \n",
    "    x,y=circle_points(source_pts[k,0], source_pts[k,1],\\\n",
    "                      3*np.sqrt(2)*source_radii[k])\n",
    "    plt.plot(x, y, 'r', linewidth=1.5)\n",
    "    \n",
    "    x,y=circle_points(target_pts[k,0]+img1.shape[1], target_pts[k,1],\\\n",
    "                      3*np.sqrt(2)*target_radii[k])\n",
    "    plt.plot(x, y, 'r', linewidth=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleSpaceBlobs(img, N):\n",
    "    start = time.time()\n",
    "    \n",
    "    sigma0 = 0.5      # The first sigma to start with\n",
    "    k = 1.19          # \n",
    "    Nscales = 18      # Number of scales in scalespace (noticable effect on execution time, you can try different values)\n",
    "    \n",
    "    # Pre-allocate memory for the scale space, sigmas and filtered images\n",
    "    scalespace = np.zeros((img.shape[0], img.shape[1], Nscales))\n",
    "    sigmas = np.zeros(Nscales)\n",
    "    tmpxx = np.zeros(img.shape)\n",
    "    tmpyy = np.zeros(img.shape)\n",
    "    \n",
    "    # Create a scalespace by...\n",
    "    print(\"Creating a scalespace...\")\n",
    "    for i in range(Nscales):\n",
    "        # Get the current sigma and generate gaussian filters\n",
    "        sigmas[i] = (k ** i) * sigma0\n",
    "        g,_,_,gxx,gyy,_, = gaussian2(sigmas[i])\n",
    "\n",
    "        # filter the image with the scale-normalized Laplacian of Gaussian\n",
    "        # for each scale i and store the result to the variable scalespace[:,:,i]\n",
    "        \n",
    "        ##-your-code-starts-here-##\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ##-your-code-ends-here-##\n",
    "        scalespace[:,:,i] = (sigmas[i]**2 * (tmpxx + tmpyy))**2\n",
    "        \n",
    "    # Selection of local maxima, each maxima defines a circular region.\n",
    "    \n",
    "    print(\"Calculating local maxima...\")\n",
    "    # Pre-allocate memory for the local maxima images\n",
    "    localmaxima = np.zeros(scalespace.shape)\n",
    "    # Filter shape for calculating the local maxima\n",
    "    footprint = np.ones((3,3))\n",
    "    footprint[1,1] = 0\n",
    "    for i in range(Nscales):\n",
    "        # Calculate local maxima\n",
    "        maxi = maximum_filter(scalespace[:,:,i], footprint=footprint, mode='constant')\n",
    "        # test if pixel values are larger than neighborhood\n",
    "        localmaxima[:,:,i] = scalespace[:,:,i] > maxi  \n",
    "      \n",
    "    # In the end each row in 'blobs' encodes one circular region as follows:.\n",
    "    # [x, y, r, filter_response]\n",
    "    # where x and y are the column and row coordinates of the circle center,\n",
    "    # r is the radius of the circle, r=sqrt(2)*sigma (see slide 77 of Lecture 3)\n",
    "    # last column indicates the response of the Laplacian of Gaussian filter\n",
    "    blobs = None\n",
    "    # Pre-allocate memory for consecutive scales\n",
    "    scaleA = np.zeros(img.shape)\n",
    "    scaleB = np.zeros(img.shape)\n",
    "    scaleC = np.zeros(img.shape)\n",
    "    \n",
    "    print(\"Calculating detections...\")\n",
    "    for i in range(1,Nscales-1):\n",
    "        # Consecutive scales\n",
    "        scaleA = scalespace[:,:,i-1]\n",
    "        scaleB = scalespace[:,:,i]\n",
    "        scaleC = scalespace[:,:,i+1]\n",
    "        # Indices of local maxima\n",
    "        ri, ci = np.nonzero(localmaxima[:,:,i])        \n",
    "        # Compare the current level to the previous and next level\n",
    "        idmax = np.nonzero((scaleA[ri,ci] < scaleB[ri,ci]) * (scaleC[ri,ci] < scaleB[ri,ci]))[0]\n",
    "        rlmax = ri[idmax]\n",
    "        clmax = ci[idmax]\n",
    "        # Add blob coordinates, circle radiuses and filter responses to 'blobs'\n",
    "        if blobs is not None:\n",
    "            tmp = np.vstack((clmax, rlmax, \n",
    "                      np.sqrt(2)*sigmas[i]*np.ones(len(rlmax)), \n",
    "                      scaleB[rlmax, clmax])).T\n",
    "            blobs = np.vstack((blobs, tmp))\n",
    "        else:\n",
    "            blobs = np.vstack((clmax, rlmax, \n",
    "                      np.sqrt(2)*sigmas[i]*np.ones(len(rlmax)), \n",
    "                      scaleB[rlmax, clmax])).T\n",
    "\n",
    "    # Sort the blobs according to the response of Laplacian of Gaussian.\n",
    "    # Return N best detections.\n",
    "    ids = np.argsort(blobs[:,3])\n",
    "    sblobs = np.flipud(blobs[ids, :])\n",
    "    blobsN = sblobs[0:min(N, sblobs.shape[0]), :]\n",
    "    # Ouput the execution time\n",
    "    print(\"Total time elapsed (s): \" + str(time.time() - start) + \"\\n\")\n",
    "\n",
    "    return blobsN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous part illustrated OpenCV lib's built-in capabilities.\n",
    "\n",
    "# Next, the task is to implement a similar blob detector as in SIFT. \n",
    "# In the example below the detections are pre-computed.\n",
    "# Since we now know the true geometric transformation H1to2p we can \n",
    "# visualize those detections from both images which have large overlap.\n",
    "# Your task is to implement the function scaleSpaceBlobs.m so that it\n",
    "# outputs similar circular regions as pre-computed in 'blobs1' and 'blobs2'.\n",
    "\n",
    "# Replace 'blobs1' and 'blobs2' below with the output of the detector.\n",
    "data=np.load(data_dir+'blobs_data.npz', encoding='latin1')\n",
    "blobs1=data['blobs1']\n",
    "blobs2=data['blobs2']\n",
    "# Each row in 'blobs1' and 'blobs2' defines a circular region as follows:  \n",
    "# [x y r filter_response]\n",
    "# here x and y are the column and row coordinates of the circle center\n",
    "# r is the radius of the circle, r=sqrt(2)*sigma (see slide 77 of Lecture 3)\n",
    "# last column indicates the response of the Laplacian of Gaussian filter\n",
    "\n",
    "# Below N is the number of strongest blobs that are returned.\n",
    "# (strongest local maxima for the scale-normalized Laplacian of Gaussian)\n",
    "# Implement scaleSpaceBlobs.\n",
    "# Everything should then work if you uncomment the following three lines and\n",
    "# turn on your \n",
    "\n",
    "#N=500;\n",
    "#blobs1 = scaleSpaceBlobs(img1, N)\n",
    "#blobs2 = scaleSpaceBlobs(img2, N)\n",
    "\n",
    "# Show detected blob features\n",
    "NVIS=50;\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
    "plt.suptitle(\"Showing all detected blobs\", fontsize=20)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(img1, cmap='gray')\n",
    "ax[1].imshow(img2, cmap='gray')\n",
    "for k in range(0, NVIS):\n",
    "    x, y = circle_points(blobs1[k,0], blobs1[k,1], 3*np.sqrt(2)*blobs1[k,2])\n",
    "    ax[0].plot(x, y, 'r', linewidth=1.5)\n",
    "    x, y = circle_points(blobs2[k,0], blobs2[k,1], 3*np.sqrt(2)*blobs2[k,2])\n",
    "    ax[1].plot(x, y, 'r', linewidth=1.5)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# below we illustrate detected regions with high overlap \n",
    "xy1to2=s*np.dot(R, blobs1[:,0:2].T)+np.tile(t,(blobs1.shape[0],1)).T\n",
    "blobs1t=np.hstack((xy1to2.T, s*np.expand_dims(blobs1[:,2],axis=1), np.expand_dims(blobs1[:,3], axis=1)))\n",
    "\n",
    "distmat = np.zeros((blobs1.shape[0], blobs2.shape[0]))\n",
    "for i in range(blobs1.shape[0]):\n",
    "    for j in range(blobs2.shape[0]):\n",
    "        distmat[i,j] = np.linalg.norm(blobs1t[i, 0:3] - blobs2[j, 0:3])\n",
    "\n",
    "dist = np.amin(distmat, axis=0)\n",
    "nnids = np.argmin(distmat, axis=0)\n",
    "sdist = np.sort(dist)\n",
    "sids = np.argsort(dist)\n",
    "idlist = np.vstack((nnids[sids], sids, sdist)).T\n",
    "\n",
    "# Visualize the 20 best matches\n",
    "Nvis = 10\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.suptitle(\"Blob detection and matching\", fontsize=20)\n",
    "\n",
    "montage = np.concatenate((img1, img2), axis=1)\n",
    "plt.imshow(montage, cmap='gray')\n",
    "plt.title('Top {} nearest neighbors of blobs features'.format(Nvis))\n",
    "\n",
    "t = np.arange(0, 2*np.pi+0.1, 0.1)\n",
    "for k in range(Nvis):\n",
    "    loc1 = blobs1[int(idlist[k, 0]), 0:2]\n",
    "    r1 = 3*np.sqrt(2)*blobs1[int(idlist[k,0]), 2]\n",
    "    loc2 = blobs2[int(idlist[k, 1]), 0:2]\n",
    "    r2 = 3*np.sqrt(2)*blobs2[int(idlist[k,1]), 2]\n",
    "    x1 = loc1[0]+r1*np.cos(t)\n",
    "    y1 = loc1[1]+r1*np.sin(t)\n",
    "    x2 = loc2[0]+r2*np.cos(t)+img1.shape[1]\n",
    "    y2 = loc2[1]+r2*np.sin(t)\n",
    "    plt.plot(x1, y1, 'm-', linewidth=3)\n",
    "    plt.plot(x2, y2, 'm-', linewidth=3)\n",
    "    plt.plot([loc1[0], loc2[0]+img1.shape[1]],[loc1[1], loc2[1]], 'c-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is just to convince you that the \n",
    "## vectorized descriptor matching implementation \n",
    "## illustrated above works correctly\n",
    "\n",
    "X = np.random.randn(5, 10)\n",
    "Y = np.random.randn(4, 10)\n",
    "\n",
    "distmat = np.dot(X,Y.T)\n",
    "X_terms = np.expand_dims(np.diag(np.dot(X, X.T)), axis=1)\n",
    "X_terms = np.tile(X_terms,(1,4))\n",
    "Y_terms = np.expand_dims(np.diag(np.dot(Y, Y.T)), axis=0)\n",
    "Y_terms = np.tile(Y_terms,(5,1))\n",
    "distmat = np.sqrt(Y_terms + X_terms - 2*distmat)\n",
    "\n",
    "print(distmat)\n",
    "\n",
    "distmat2 = np.zeros((X.shape[0], Y.shape[0]))\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(Y.shape[0]):\n",
    "        distmat2[i,j] = np.linalg.norm(X[i,:] - Y[j,:])\n",
    "\n",
    "print(distmat2)\n",
    "\n",
    "print(np.sum(distmat-distmat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
