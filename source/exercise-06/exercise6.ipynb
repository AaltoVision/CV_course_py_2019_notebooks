{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "#   Exercise6 notebook.\n",
    "#\n",
    "# Copyright (C) 2018 Santiago Cortes, Juha Ylioinas\n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later); please refer to the file \n",
    "# Licence.txt, included with the software, for details.\n",
    "\n",
    "\n",
    "# Preparations\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "from itertools import compress\n",
    "\n",
    "from scipy.ndimage import maximum_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import convolve1d as conv1\n",
    "from scipy.ndimage.filters import convolve as conv2\n",
    "#from scipy.misc import imresize, imsave\n",
    "\n",
    "from skimage.transform import ProjectiveTransform, SimilarityTransform, AffineTransform\n",
    "from skimage.measure import ransac\n",
    "from skimage.io import imread\n",
    "\n",
    "from utils import rgb2gray, matchFeatures, imwarp, blend\n",
    "\n",
    "import time\n",
    "\n",
    "# Select data directory\n",
    "\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-06-data/')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 6\n",
    "This is a python demo of panoramic image stitching (no points given). Remember to do the pen and paper assignments given in Exercise06.pdf.\n",
    "\n",
    "The example demonstrates robust\n",
    "feature-based panorama stitching, which was covered during the Lecture 5 (i.e. SURF\n",
    "feature extraction and matching and RANSAC based homography estimation). Homography estimation is also covered in Chapter 4 of the book by Hartley & Zisserman. <br><br> \n",
    "Check the example by running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo - Feature-Based Panoramic Image Stiching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Feature detection and matching are used in many computer vision applications such as image registration, tracking, and object detection. This example illustrates how feature based techniques are used to automatically stitch together a set of images. Indeed, the procedure is just an extension of feature based image registration -- instead of registering a single pair of images, multiple image pairs are successively registered relative to each other to form a panorama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Load and Show Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all images in the given directory\n",
    "# You can try with your own images, but make sure that they are named\n",
    "# so that the glob collects them in left-to-right orde\n",
    "imgfiles = np.sort(glob.glob(data_dir+'*.jpg'))\n",
    "\n",
    "imgs = [imread(imgfile) for imgfile in imgfiles]\n",
    "   \n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(imgs), figsize=(16,8))\n",
    "ax = axes.ravel()\n",
    "\n",
    "for n in range(len(imgs)):\n",
    "    ax[n].imshow(imgs[n])\n",
    "    ax[n].set_title('Input image {}'.format(n+1))\n",
    "    ax[n].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Register Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the first image to gray-scale\n",
    "grayimg = rgb2gray(imgs[0]).astype(np.uint8)\n",
    "\n",
    "# Initiate SURF detector (use this if you have opencv compiled with opencv-contrib)\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT detector from img \n",
    "kpt, desc = surf.detectAndCompute(grayimg, None)\n",
    "kpt = np.array([p.pt for p in kpt]) # make a numpy array from keypoint locations\n",
    "\n",
    "## Shift should work this year (2019)-> Code below should not be needed. --------------------------------\n",
    "# ..use precomputed features if you don't have opencv compiled with opencv-contrib\n",
    "\n",
    "## Using a trick to circumvent a bug in the new version of np.load\n",
    "## save np.load\n",
    "#np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "#data = np.load(data_dir+'department1_surf_kpts_descrs.npy', encoding='latin1')\n",
    "\n",
    "#kpt = data.item().get('keypoints')\n",
    "#desc = data.item().get('descriptors')\n",
    "## ------------------------------------------------------------------------------------------\n",
    "\n",
    "## Initialize all the transforms to the identity matrix. We use\n",
    "## projective transform here because the department images are fairly\n",
    "## close to the camera. If the scene was from a further distance,\n",
    "## an affine transformation would be enough.\n",
    "numImages = len(imgs)\n",
    "tforms = [np.eye(3)]\n",
    "\n",
    "# Initialize variable to hold image sizes.\n",
    "imageSize = np.zeros((numImages,2));\n",
    "\n",
    "fig, axes = plt.subplots(nrows=numImages-1, ncols=1, figsize=(32,16))\n",
    "ax = axes.ravel()\n",
    "\n",
    "# Iterate over remaining image pairs\n",
    "for n in range(1, numImages):\n",
    "    # Store points and features for I(n-1)\n",
    "    grayimgPrevious = grayimg\n",
    "    kptPrevious = kpt;\n",
    "    descPrevious = desc;\n",
    "\n",
    "    # Convert image I(n) to grayscale\n",
    "    grayimg = rgb2gray(imgs[n]).astype(np.uint8)\n",
    "\n",
    "    # Save image size\n",
    "    imageSize[n,:] = np.shape(grayimg);\n",
    "\n",
    "    # Detect and extract SURF features for I(n) (use this if you have opencv compiled with opencv-contrib)\n",
    "    kpt, desc = surf.detectAndCompute(grayimg, None)\n",
    "    kpt = np.array([p.pt for p in kpt]) # make a numpy array from keypoint locations\n",
    "    \n",
    "    ## Shift should work this year (2019)-> Code below should not be needed. --------------------------------\n",
    "    # ..use precomputed features if you don't have opencv compiled with opencv-contrib\n",
    "    #data = np.load(data_dir+'department{}_surf_kpts_descrs.npy'.format(n+1), encoding='latin1')\n",
    "    #kpt = data.item().get('keypoints')\n",
    "    #desc = data.item().get('descriptors')\n",
    "    ## ------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Find correspondences between I(n) and I(n-1)\n",
    "    indexPairs = matchFeatures(desc, descPrevious);\n",
    "\n",
    "    matchedPoints = kpt[indexPairs[:,0],:]\n",
    "    matchedPointsPrev = kptPrevious[indexPairs[:,1], :]\n",
    "\n",
    "    # Estimate the transformation between I(n) and I(n-1).\n",
    "    tform, inliers = ransac((matchedPoints, matchedPointsPrev), ProjectiveTransform,\\\n",
    "                         min_samples=4, residual_threshold=1.5, max_trials=2000)\n",
    "    tforms.append(tform.params)\n",
    "    \n",
    "    # Compute T(n) * T(n-1) * ... * T(1)\n",
    "    tform_ = np.dot(tforms[n],tforms[n-1])\n",
    "    tforms[n] = tform_\n",
    "\n",
    "    # plots\n",
    "    montage = np.concatenate((grayimgPrevious,grayimg), axis=1)\n",
    "    ax[n-1].imshow(montage, cmap='gray')\n",
    "    ax[n-1].set_title(\"matches between img{}.jpg and img{}.jpg\".format(n, n+1))\n",
    "    mpin = matchedPoints[inliers,:]\n",
    "    mprevin = matchedPointsPrev[inliers,:]\n",
    "    for k in range(mpin.shape[0]):\n",
    "        ax[n-1].plot([mprevin[k,0], mpin[k,0]+grayimgPrevious.shape[1]], \n",
    "                 [mprevin[k,1], mpin[k,1]])\n",
    "    \n",
    "## restore np.load for future normal usage\n",
    "#np.load = np_load_old   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Initialize Panorama Image\n",
    "Currently, all the transformations in tforms are relative to the first image. This is as it was very convenient to code the image registration procedure using this kind of sequential processing of all the images (as you might have noticed). Using the first image as the start of the panorama, however, does not produce the most aesthetically pleasing panorama. This is because it tends to distort most of the images that form the panorama. A more satisfactory panorama can be created by modifying the transformations such that the center of the scene is the least distorted. This is done by inverting the transformation for the center image and applying this transform to all the others.\n",
    "\n",
    "Start by using the outputLimits method to find the output limits for each transform. The output limits are then used to create an initial, empty, panorama into which all the transformed images are mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputLimits(tform, img_width, img_height):\n",
    "    xv,yv = np.meshgrid(np.arange(1,img_width),np.arange(1,img_height))\n",
    "    y = np.dot(tform, np.vstack((xv.flatten(), yv.flatten(), np.ones((1, xv.size)))))\n",
    "    y_ = y[:2] / y[2]\n",
    "    y_ = y_.T\n",
    "    x_min, y_min = np.amin(y_, axis=0)\n",
    "    x_max, y_max = np.amax(y_, axis=0)\n",
    "    xlim = [x_min, x_max]\n",
    "    ylim = [y_min, y_max]\n",
    "    return xlim, ylim\n",
    "\n",
    "# Compute the output limits 'xlims' and 'ylims' for each transform\n",
    "sel = int(np.floor(numImages/2))\n",
    "\n",
    "Tinv = np.linalg.inv(tforms[sel])\n",
    "\n",
    "for i in range(len(tforms)):\n",
    "    tform_ = np.dot(tforms[i], Tinv)\n",
    "    tforms[i] = tform_\n",
    "\n",
    "xlims = []\n",
    "ylims = []\n",
    "for i in range(len(tforms)):\n",
    "    xlim, ylim = outputLimits(tforms[i], imgs[i].shape[1], imgs[i].shape[0]);\n",
    "    xlims.append(xlim)\n",
    "    ylims.append(ylim)\n",
    "xlims = np.array(xlims)\n",
    "ylims = np.array(ylims)\n",
    "\n",
    "panorama = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Create Panorama image\n",
    "Use imwarp to map images into the panorama and use the blend function to overlay the images together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# Create the panorama\n",
    "for n in range(len(tforms)):\n",
    "    img = imgs[n]\n",
    "    tform = tforms[n]\n",
    "    \n",
    "    # Transform img into the panorama.\n",
    "    imgt = imwarp(img, tform, xlims, ylims)\n",
    "    \n",
    "    # Make a mask and transform that as well\n",
    "    mask = np.ones(img.shape[:2])\n",
    "    maskt = imwarp(mask, tform, xlims, ylims)\n",
    "    \n",
    "    # Overlay the warpedImage onto the panorama.\n",
    "    if panorama is not None:\n",
    "        panorama = blend(imgt, maskt, panorama)\n",
    "    else:\n",
    "        panorama = np.zeros((imgt.shape[0], imgt.shape[1], 3))\n",
    "        panorama = blend(imgt, maskt, panorama)\n",
    "        \n",
    "plt.imshow(panorama)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The example demonstrated how to automatically create a panorama using feature detection and matching and image registration techniques. To improve the result, additional techniques of blending and alignment of images can be incorporated into the example [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Matthew Brown and David G. Lowe. 2007. Automatic Panoramic Image Stitching using Invariant Features. Int. J. Comput. Vision 74, 1 (August 2007), 59-73."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
