{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparations\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import scipy\n",
    "from itertools import compress\n",
    "\n",
    "from scipy.ndimage import maximum_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import convolve1d as conv1\n",
    "from scipy.ndimage.filters import convolve as conv2\n",
    "\n",
    "from skimage.transform import resize, rotate, SimilarityTransform\n",
    "from skimage.io import imread\n",
    "from skimage.measure import ransac\n",
    "\n",
    "from utils import plotFrame, plotFrameBoth, plotImBoth, findNeighbours, plotMatches, geometricVerification\n",
    "\n",
    "import time\n",
    "\n",
    "# Select data directory\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-07-data')\n",
    "print('Data stored in %s' % data_dir)\n",
    "\n",
    "def getFeatures(imname, postfix=\"\"):\n",
    "    data = np.load(data_dir+'/data_part1/{}_sift_disc_kps_descrs{}.npy'.format(imname,postfix), encoding='latin1', allow_pickle=True)\n",
    "    kps = data.item().get('keypoints')\n",
    "    descrs = data.item().get('descriptors')\n",
    "    kps = kps.T\n",
    "    descrs = descrs.T\n",
    "    return kps, descrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "#    Exercise7 VGG practical notebook (stage1, fast track).\n",
    "#\n",
    "# This software is inspired by original object instance recognition\n",
    "# VGG practical. \n",
    "# Licence; please refer to the file \n",
    "# Licence.txt, included with the software, for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 7\n",
    "\n",
    "This is a minimal python version of Oxford Visual Geometry Group's Matlab practical on recognition of object instances\n",
    "(see the original webpage <a href= http://www.robots.ox.ac.uk/~vgg/practicals/instance-recognition/index.html#part1>here</a>).\n",
    "By \"minimal\" it is meant that it uses pre-computed SIFT features and some other needed resources.\n",
    "All of them are calculated using the freely available matlab scripts found in the practical's github <a href=https://github.com/vedaldi/practical-object-instance-recognition >repository</a>.\n",
    "The practical is largely based on the vlfeat library (cf. http://www.vlfeat.org/) which unfortunately does not have a Python interface. \n",
    "<br><br> \n",
    "This notebook is <b>the first part (PART I)</b> of the practical on the so-called  <em> fast track</em> and is about <b>sparse features for matching object instances</b>.\n",
    "<br><br>\n",
    "Go through the notebook and answer the questions. You can write your answers to a separate text document and submit that as you are not supposed to implement anything in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Sparse features for matching object instances\n",
    "## Stage I.A: SIFT features detector\n",
    "The SIFT feature has both a detector and a descriptor. We will start by computing and visualizing the SIFT feature detections for two images of the same object (a building facade). Load an image, rotate and scale it, and then display the original and transformed pair:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------\n",
    "#                                   Stage I.A: SIFT features detection\n",
    "# --------------------------------------------------------------------\n",
    "# Load an image\n",
    "im1 = imread(data_dir+'/data/oxbuild_lite/all_souls_000002.jpg') / 255.\n",
    "\n",
    "# Let the second image be a rotated and scaled version of the first\n",
    "im3_ = rotate(im1, 35, 'bilinear')\n",
    "im3_shape = np.round((0.7 * im3_.shape[0], 0.7 * im3_.shape[1]))\n",
    "im3 = resize(im3_, im3_shape)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "ax.set_title(\"Part I.A: Original image and rotated and scaled version\", fontsize=20)\n",
    "plotImBoth(ax, im1, im3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SIFT frame is a circle with an orientation and is specified by four parameters: the center $t_{x}$, $t_{y}$, the scale $s$, and the rotation $\\theta$ (in radians), resulting in a vector of four parameters ($s$,$\\theta$,$t_{x}$,$t_{y}$). Now compute and visualise the SIFT feature detections (frames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SIFT features for each (precomputed) \n",
    "# In our precomputed features the 'peakThreshold' param is set to 0.01 \n",
    "# like in the original matlab version\n",
    "kps1, descrs1 = getFeatures('im1')\n",
    "kps3, descrs3 = getFeatures('im3rotscaled')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "ax.set_title(\"Part I.A: SIFT features detection - synthetic pair\", fontsize=20)\n",
    "plotFrameBoth(ax, im1, im3, kps1, kps3, [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the second image and its rotated and scaled version and convince yourself that the detections overlap the same scene regions (even though the circles have moved their image position and changed radius). You may try to see this better by enlarging the image pair. This demonstrates that the detection process transforms (is co-variant) with translations, rotations and isotropic scalings. This class of transformations is known as a similarity or equiform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: The number of detected features can be controlled by changing the peakThreshold option. A larger value will select features that correspond to higher contrast structures in the image. Try by uncommenting the cell code lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SIFT features for each (precomputed), set peakThreshold as 0.05\n",
    "#kps1_, descrs1_ = getFeatures('im1','_0_05')\n",
    "#kps3_, descrs3_ = getFeatures('im3rotscaled', '_0_05')\n",
    "\n",
    "#fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "#ax.set_title(\"Part I.A: SIFT features detection - synthetic pair\\n (with peakThreshold set to 0.05)\", fontsize=20)\n",
    "#plotFrameBoth(ax, im1, im3, kps1_, kps3_, [])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the exercise with a pair of natural images. Start by loading another image. and plot images and feature frames. Again you should see that many of the detections overlap the same scene region. Note that, while repeatability occurs for the pair of natural views, it is much better for the synthetically rotated pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second image of the same scene\n",
    "im2 = imread(data_dir+'/data/oxbuild_lite/all_souls_000015.jpg') / 255.\n",
    "\n",
    "# Compute SIFT features for each (precomputed)\n",
    "# peakThreshold was 0.01\n",
    "kps2, descrs2 = getFeatures('im2')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10,15))\n",
    "ax = axes.ravel()\n",
    "ax[0].set_title(\"Part I.A: Original images - real pair\", fontsize=20)\n",
    "plotImBoth(ax[0], im1, im2)\n",
    "ax[1].set_title(\"Part I.A: SIFT features detection - synthetic pair\", fontsize=20)\n",
    "plotFrameBoth(ax[1], im1, im2, kps1, kps2, [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question</b>: Note the change in density of detections across the image. Why does it change? Will it be a problem for matching? How could it be avoided?\n",
    "\n",
    "<b>Question</b>: Occasionally, a feature is detected multiple times, with different orientations. This may happen when the orientation assignment is ambiguous. Which kind of image structure would result in ambiguous orientation assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage I.B: SIFT features detectors and matching between images\n",
    "Next we will use the descriptor computed over each detection to match the detections between images. We will start with the simplest matching scheme (first nearest neighbour of descriptors) and then add more sophisticated methods to eliminate any mismatches.\n",
    "\n",
    "- Visualize the SIFT descriptors for the detected feature frames by plotting the precomputed descriptor frames. Then use plotframe to overlay the corresponding frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescriptorFramesAndKeypoints():\n",
    "    data = np.load(data_dir+'/data_part1/im1_sift_descriptor_xyall_every50th.npy', encoding='latin1', allow_pickle=True)\n",
    "    xall = data.item().get('xall')\n",
    "    yall = data.item().get('yall')\n",
    "\n",
    "    data = np.load(data_dir+'/data_part1/im1_sift_disc_kps_every50th.npy', encoding='latin1', allow_pickle=True)\n",
    "    kps = data.item().get('keypoints')\n",
    "    kps = kps.T\n",
    "    return xall, yall, kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SIFT descriptors (only a few)\n",
    "xall, yall, kps = getDescriptorFramesAndKeypoints()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "# Plot the SIFT descriptor (precomputed) regions of interest for the detected feature frames\n",
    "ax.plot(xall, yall, color='b')\n",
    "# Overlay the corresponding frames\n",
    "plotFrame(ax, im1, kps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question</b>: Note the descriptors are computed over a much larger region (shown in blue) than the detection (shown in green). Why?\n",
    "\n",
    "Hint: Compare the two largest rectangles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute first nearest neighbours matches - for each SIFT descriptor in the first image, compute its nearest neighbour in the second image with the function findNeighbours.\n",
    "\n",
    "- Visualize the correspondences using lines joining matched SIFT features with the function plotFrameBoth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SIFT features for each (precomputed)\n",
    "kps2, descrs2 = getFeatures('im2')\n",
    "\n",
    "# Find for each descriptor in im1 the closest descriptor in im2\n",
    "# have a look at the findNeighbours in utils.py\n",
    "ind, dist = findNeighbours(descrs1, descrs2, 1)\n",
    "\n",
    "matches_raw = np.vstack((np.arange(descrs1.shape[0]), ind))\n",
    "matches_raw = matches_raw.T\n",
    "\n",
    "# Display correspondences\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "plotFrameBoth(ax, im1, im2, kps1, kps2, matches_raw, plotMatches=True)\n",
    "ax.set_title(\"Part I.B: SIFT descriptors - matching\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question</b>: Notice that there are many mismatches. Examine some of the mismatches to understand why the mistakes are being made. For example, is the change in lighting a problem? What additional constraints can be applied to remove the mismatches?\n",
    "\n",
    "Hint: You can visualize a subset of the matches by uncommenting the lines below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some guidance to the second question ...\n",
    "#fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "#plotFrameBoth(ax, im1, im2, kps1, kps2, matches_raw[::20,:], plotMatches=True)\n",
    "#ax.set_title(\"Part I.B: SIFT descriptors - matching\", fontsize=20)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage I.C: Improving SIFT matching using Lowe’s second nearest neighbour test\n",
    "Lowe introduced a second nearest neighbour (2nd NN) test to identify, and hence remove, ambiguous matches. The idea is to identify distinctive matches by a threshold on the ratio of first to second NN distances. In the cell below, the ratio is nnThreshold = 1NN distance / 2NN distance.\n",
    "\n",
    "- Vary the ratio nnThreshold in a range from 0.1 to 0.9, and examine how the number of matches and number of mismatches changes.\n",
    "- A value of nnThreshold = 0.8 is often a good compromise between losing too many matches and rejecting mismatches.\n",
    "\n",
    "<b>Question</b>: Examine some of the remaining mismatches to understand why they have occurred. How could they be removed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------\n",
    "#   Stage I.C: Better matching w/ Lowe's second nearest neighbour test\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Find the top two neighbours as well as their distances\n",
    "ind, dist = findNeighbours(descrs1, descrs2, 2)\n",
    "\n",
    "# Accept neighbours if their second best match is sufficiently far off\n",
    "nnThreshold = 0.8\n",
    "ratio2 = np.divide(dist[:,0], dist[:,1])\n",
    "ok = ratio2 <= nnThreshold ** 2\n",
    "\n",
    "# Construct a list of filtered matches\n",
    "matches_2nn = np.vstack((np.nonzero(ok), ind[ok, 0]))\n",
    "matches_2nn = matches_2nn.T\n",
    "\n",
    "# Display the corresponding matches\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "plotFrameBoth(ax, im1, im2, kps1, kps2, matches_2nn, plotMatches=True)\n",
    "ax.set_title(\"Part I.C: SIFT descriptors - Lowe's test\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage I.D: Improving SIFT matching using a geometric transformation\n",
    "In addition to the 2nd NN test, we can also require consistency between the matches and a geometric transformation between the images. For the moment we will look for matches that are consistent with a similarity transformation\n",
    "\n",
    "which consists of a rotation by θ, an isotropic scaling (i.e. same in all directions) by s, and a translation by a vector $(t_{x},t_{y})$. This transformation is specified by four parameters $(s,\\theta,t_{x},t_{y})$ and can be computed from a single correspondence between SIFT detections in each image.\n",
    "\n",
    "<b>Task</b>: Work out how to compute this transformation from a single correspondence.\n",
    "     \n",
    "\n",
    "Hint: Recall from Stage I.A that a SIFT feature frame is an oriented circle and map one onto the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometric verification\n",
    "inliers, H = geometricVerification(kps1, kps2, matches_2nn, 8)\n",
    "matches_geom = matches_2nn[inliers,:]\n",
    "\n",
    "# Display the matches\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,15))\n",
    "plotFrameBoth(ax, im1, im2, kps1, kps2, matches_geom, plotMatches=True)\n",
    "ax.set_title(\"Part I.C: SIFT descriptors - geometric verification\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches consistent with a similarity can then be found using a RANSAC inspired algorithm, implemented by the function geometricVerification: \n",
    "\n",
    "RANSAC-like algorithm for geometric verification\n",
    "\n",
    "    1. For each tentative correspondence in turn:\n",
    "        1. compute the similarity transformation;\n",
    "        2. map all the SIFT detections in one image to the other using this transformation;\n",
    "        3. accept matches that are within a threshold distance to the mapped detection (inliers);\n",
    "        4. count the number of accepted matches;\n",
    "        5. optionally, fit a more accurate affine transformation or homography to the accepted matches and test re-validate the matches.\n",
    "    2. Choose the transformation with the highest count of inliers.\n",
    "\n",
    "After this algorithm the inliers are consistent with the transformation and are retained, and most mismatches should now be removed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
