{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparations\n",
    "import os\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "from itertools import compress\n",
    "from pyflann import *\n",
    "\n",
    "from scipy.ndimage import maximum_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import convolve1d as conv1\n",
    "from scipy.ndimage.filters import convolve as conv2\n",
    "#from scipy.misc import imresize, imsave\n",
    "from scipy import sparse\n",
    "\n",
    "from skimage.transform import resize, rotate, SimilarityTransform\n",
    "from skimage.io import imread\n",
    "from skimage.measure import ransac\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from utils import findNeighbours, kdtreequery, matchWords, plotMatches, geometricVerification, getHistogramFromDescriptor, plotRetrievedImages, plotFrameBoth\n",
    "\n",
    "import time\n",
    "\n",
    "# Select data directory\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-07-data')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "#    Exercise7 VGG practical notebook (Part3, fast track).\n",
    "#\n",
    "# This software is inspired by original object instance recognition\n",
    "# VGG practical. \n",
    "# Licence; please refer to the file \n",
    "# Licence.txt, included with the software, for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 7\n",
    "\n",
    "This is a minimal python version of Oxford Visual Geometry Group's Matlab practical on recognition of object instances (see the original webpage <a href= http://www.robots.ox.ac.uk/~vgg/practicals/instance-recognition/index.html#part3>here</a>). By \"minimal\" it is meant that it uses pre-computed SIFT features and some other needed resources. All of them are calculated using the freely available matlab scripts found in the practical's github <a href=https://github.com/vedaldi/practical-object-instance-recognition >repository</a>. The practical is largely based on the vlfeat library (cf. http://www.vlfeat.org/) which unfortunately does not have a Python interface. <br><br>\n",
    "\n",
    "This notebook is <b>the third part (PART III)</b> of the practical on the so-called <em>fast-track</em> and demonstrates the operation of <b>a large scale image retriaval system</b>.\n",
    "<br><br>\n",
    "Go through the notebook and answer the questions. You can write your answers to a separate text document and submit that as you are not supposed to implement anything in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Towards large scale retrieval\n",
    "\n",
    "In large scale retrieval the goal is to match a query image to a large database of images (for example the WWW or Wikipedia). The quality of a match is measured as the number of geometrically verified feature correspondences between the query and a database image. While the techniques discussed in Part I and II are sufficient to do this, in practice they require too much memory to store the SIFT descriptors for all the detections in all the database images. We explore next two key ideas: one to reduce the memory footprint and pre-compute descriptor matches; the other to speed up image retrieval.\n",
    "\n",
    "Task: Examine and run the python code lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------\n",
    "#      Preparations\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Load images (rgb2gray is used here just to easen plotting)\n",
    "im1 = imread(data_dir+'/data/oxbuild_lite/ashmolean_000007.jpg') / 255.\n",
    "im2 = imread(data_dir+'/data/oxbuild_lite/ashmolean_000028.jpg') / 255.\n",
    "\n",
    "## Load precomputed data \n",
    "# SIFT feature points and descriptors for im1 and im2, and quantized descriptors' word vocabulary\n",
    "data1=np.load(data_dir+\"/data_part3/img1_sift_kps_descs.npy\", encoding='latin1', allow_pickle=True)\n",
    "kps1 = data1.item().get('keypoints')\n",
    "descrs1 = data1.item().get('descriptors')\n",
    "\n",
    "data2=np.load(data_dir+\"/data_part3/img2_sift_kps_descs.npy\", encoding='latin1', allow_pickle=True)\n",
    "kps2 = data2.item().get('keypoints')\n",
    "descrs2 = data2.item().get('descriptors')\n",
    "\n",
    "# REMEMBER TO DOWNLOAD THE FILE USING THE GDRIVE LINK \n",
    "vocab = np.load(data_dir+\"/data_part3/sift_disc_vocab.npy\", encoding='latin1', allow_pickle=True)\n",
    "\n",
    "# Get the matches based on the raw descriptors\n",
    "tic = time.clock()\n",
    "#%lprun -f findNeighbours findNeighbours(descrs1, descrs2, numNeighbors=2)\n",
    "ind, dist = findNeighbours(descrs1, descrs2, numNeighbors=2)\n",
    "nnThreshold = 0.85\n",
    "ratio2 = np.divide(dist[:,0], dist[:,1])\n",
    "ok = ratio2 <= nnThreshold ** 2\n",
    "matches_raw = np.vstack((np.nonzero(ok), ind[ok, 0]))\n",
    "matches_raw = matches_raw.T\n",
    "time_raw = time.clock()-tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage III.A: Accelerating descriptor matching with visual words\n",
    "Instead of matching feature descriptors directly as done in Part I and II, descriptors are usually mapped first to discrete symbols, also called visual words, by means of a clustering technique like K-Means. The descriptors that are assigned to the same visual word are considered matched. Each of the rows in the following figure illustrates image patches that are mapped to the same visual word, and are hence indistinguishable by the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imvw = imread(data_dir+'/data/visualwords.png')\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\n",
    "ax.imshow(imvw)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, matching two sets of feature descriptors (from two images) reduces to finding the intersection of two sets of symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "- Load a visual word dictionary (should be done already) and familiarize yourself with the associated approximate nearest neighbour (ANN) matcher inside the kdtreequery function <br>(the ANN matcher is used to determine the closest visual word to each descriptor and is based on a forest of KD trees).\n",
    "- Given SIFT descriptors for two images, quantise them (assign them) into the corresponding visual words.\n",
    "- Find corresponding features by looking for the same visual words in the two images and note the computation time.\n",
    "- Geometrically verify these initial correspondences and count the number of inlier matches found.\n",
    "- Find corresponding features by using the method of Part I and II, i.e. by comparing the descriptors directly, <br>\n",
    "and note the computation time. Geometrically verify these initial correspondences and count the number of inlier matches found.\n",
    "- Compare the speed and number of inliers when using visual words vs raw SIFT descriptors by means of the function matchWords. <br>Note, you should repeat the timing (by running the matching again) <br>as the first time you run it there may be a delay as a result of running the code using jupyter notebooks.\n",
    "- Optional: compare the speed and number of matches over another pair of images (from part I and II)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------\n",
    "#      Stage III.a: Accelerating descriptor matching with visual words\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Quantise the descriptors\n",
    "words1 = kdtreequery(vocab, descrs1, maxNumChecks=1024)\n",
    "words2 = kdtreequery(vocab, descrs2, maxNumChecks=1024)\n",
    "\n",
    "# Get the matches based on quantized descriptors\n",
    "tic = time.clock()\n",
    "#%lprun -f matchWords matchWords(words1, words2)\n",
    "matches_word = matchWords(words1, words2)\n",
    "time_word = time.clock()-tic\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(32,16))\n",
    "ax = axes.ravel()\n",
    "plotFrameBoth(ax[0], im1, im2, kps1, kps2, matches_raw, plotMatches=True)\n",
    "ax[0].set_title(\"Matches on raw descriptors ({} matches in {} seconds)\".format(len(matches_raw), time_raw))\n",
    "plotFrameBoth(ax[1], im1, im2, kps1, kps2, matches_word, plotMatches=True)\n",
    "ax[1].set_title(\"Matches on quantized descriptors ({} matches in {} seconds)\".format(len(matches_word), time_word))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count inliers, i.e. perform geometric verification\n",
    "inliers_raw, H_raw = geometricVerification(kps1, kps2, matches_raw, 3)\n",
    "inliers_word, H_word = geometricVerification(kps1, kps2, matches_word, 3)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(32,16))\n",
    "ax = axes.ravel()\n",
    "plotFrameBoth(ax[0], im1, im2, kps1, kps2, matches_raw[inliers_raw,:], plotMatches=True)\n",
    "ax[0].set_title(\"Verified matches on raw descriptors ({} matches)\".format(len(matches_raw[inliers_raw,:])))\n",
    "plotFrameBoth(ax[1], im1, im2, kps1, kps2, matches_word[inliers_word,:], plotMatches=True)\n",
    "ax[1].set_title(\"Verified matches on quantized descriptors ({} matches)\".format(len(matches_word[inliers_word,:])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Questions</b>:\n",
    "- The size of the vocabulary (the number of clusters) is an important parameter in visual word algorithms. How does the size affect the number of inliers and the difficulty of computing the transformation?\n",
    "- In the above procedure the time required to convert the descriptors into visual words was not accounted for. Why?\n",
    "- What is the speedup in searching a large, fixed database of 10, 100, 1000 images? \n",
    "\n",
    "Hint: You may use the lineprofiler (commented lines) to better see what is happening behind the curtains (uncomment and compare Total times). The timings reported in the titles of the plots are not very truthful, but there is a difference between those numbers as well). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step III.b: Searching with an inverted index\n",
    "While matching with visual words is much faster than doing so by comparing feature descriptors directly, scoring images directly based on the number of geometrically verified matches still entails fitting a geometric model, a relatively slow operation. Rather than scoring all the images in the database in this way, we are going to use an approximation and count the number of visual words shared between two images.\n",
    "\n",
    "To this end, one computes a histogram of the visual words in a query image and for each of the database images. Then the number of visual words in common can be computed from the intersection of the two histograms.\n",
    "\n",
    "The histogram intersection can be thought as a similarity measure between two histograms. In practice, this measure can be refined in several ways:\n",
    "- By reducing the importance of common visual words. This is similar to a stop-words list and can be implemented by weighting each word by the `inverse document frequency' (the inverse of the frequency of occurrence of that visual word over the entire database of images).\n",
    "- By normalising the weighted histograms to unit vectors and using the cosine between them as similarity. This can be implemented easily as the inner product between normalised histograms.\n",
    "\n",
    "Computing histogram similarities can be implemented extremely efficiently using an inverted file index. In this exercise, inner products between normalized histograms are computed quite efficiently using scipy's built-in sparse matrix engine.\n",
    "\n",
    "We now apply this retrieval method to search using a query image within a 660 image subset of the Oxford 5k building image set.\n",
    "\n",
    "<b>IMPORTANT NOTE</b> (after you have run the lines in the cell below):The top image does not have here (in this python practical) a score of 1 (but ~0.97 instead). This is because the computations for the database images were entirely (including descriptor quantization) done in Matlab using the VLfeat library. For the query, in turn, the words are generated using a pyflann-based implementation (check utils.py). The difference in scores reflects the fact that the given descriptor quantization methods (here and the one in the matlab practical) produce both a bit different quantization result with the same vocabulary and descriptors. If the words of the database images were generated in a similar manner than for the query the top score would be 1 (as it is the same image), or other way around, it would be 1 if the words for the query were also generated using the matlab version. Now, your task is just to pretend that the score was 1 (like it should be in reality) and answer the question. <em>If you want to compare the generated short-list here to the one generated by the original practical in Matlab (see ./data/retrieval-shortlist.png), you'll see that it contains the same images but with a bit varying ordering, the most strongly matching being the same.</em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------\n",
    "#                        Stage III.B: Searching with an inverted index\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# We are using im2 as the query.\n",
    "\n",
    "## Load precomputed data \n",
    "# SIFT feature points and descriptors for im2 (note that we are using affine robust SIFT features)\n",
    "data2 = np.load(data_dir+\"/data_part3/img2_sift_ellipse_kps_descs.npy\", encoding='latin1', allow_pickle=True)\n",
    "kps2 = data2.item().get('keypoints')\n",
    "descrs2 = data2.item().get('descriptors')\n",
    "\n",
    "# Load all imdb related resources\n",
    "data = np.load(data_dir+\"/data_part3/imdb.npy\", encoding='latin1', allow_pickle=True)\n",
    "\n",
    "index = data.item().get('index') # all sift feature histograms of the database images\n",
    "vocab = data.item().get('vocabulary') # visual word vocabulary (ellipse)\n",
    "idf = data.item().get('idf') # inverse document frequency \n",
    "imdbImDir = str.encode(data_dir+'/')+data.item().get('imdir') # imgfile information\n",
    "imdbImNames = data.item().get('imnames')\n",
    "imdbImWords = data.item().get('imwords') # quantized sift descriptors\n",
    "imdbImKps = data.item().get('imkps') # keypoint frames of database images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = getHistogramFromDescriptor(vocab, idf, descrs2)\n",
    "\n",
    "# Score the other images by similarity to the query\n",
    "tic = time.clock()\n",
    "scores = index.dot(h)\n",
    "time_index = time.clock() - tic\n",
    "scores = scores.squeeze()\n",
    "\n",
    "plotRetrievedImages(imdbImDir, imdbImNames, scores, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 7\n",
    "\n",
    "This is a minimal python version of Oxford Visual Geometry Group's Matlab practical on recognition of object instances (see the original webpage <a href= http://www.robots.ox.ac.uk/~vgg/practicals/instance-recognition/index.html#part3>here</a>). By \"minimal\" it is meant that it uses pre-computed SIFT features and some other needed resources. All of them are calculated using the freely available matlab scripts found in the practical's github <a href=https://github.com/vedaldi/practical-object-instance-recognition >repository</a>. The practical is largely based on the vlfeat library (cf. http://www.vlfeat.org/) which unfortunately does not have a Python interface. \n",
    "<br><br> \n",
    "This notebook is <b>the first part (PART I)</b> of the practical on the so-called  <em> fast track</em> and is about <b>sparse features for matching object instances</b>.\n",
    "<br><br>\n",
    "Go through the notebook and answer the questions. You can write your answers to a separate text document and submit that as you are not supposed to implement anything in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: How many erroneously matched images do you count in the top results? <br> <b>Question</b>: Why does the top image have a score of 1 (0.9698... see the NOTE above)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage III.C: Geometric rescoring\n",
    "Histogram-based retrieval results are good but far from perfect. Given a short list of top ranked images from the previous step, we are now going to re-score them based on the number of inlier matches after a geometric verification step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------------------------------------------------------\n",
    "#                                    Stage III.C: Geometric reranking\n",
    "# --------------------------------------------------------------------\n",
    "# Rescore the top 25 images based on the number of\n",
    "# inlier matches.\n",
    "sorted_scores = np.sort(scores, axis=0)\n",
    "id_sorted_scores = np.argsort(scores, axis=0)\n",
    "sorted_scores = sorted_scores[::-1]\n",
    "id_sorted_scores = id_sorted_scores[::-1]\n",
    "\n",
    "words = kdtreequery(vocab, descrs2, maxNumChecks=1024)\n",
    "\n",
    "for rank in range(25):\n",
    "    ii = id_sorted_scores[rank]\n",
    "    db_instance_words = imdbImWords[ii]\n",
    "    matches = matchWords(words, db_instance_words)\n",
    "    instance_kps = imdbImKps[ii].T\n",
    "    inliers_word, H_word = geometricVerification(kps2, instance_kps, matches, 3)\n",
    "    newScore = len(inliers_word)\n",
    "    scores[id_sorted_scores[rank]] = newScore\n",
    "\n",
    "# Plot results by decreasing score\n",
    "plotRetrievedImages(imdbImDir, imdbImNames, scores, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question</b>: Why is the top score much larger than 1 now?<br> <b>Question</b>: Are the retrieval results improved after geometric verification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage III.D: Full system\n",
    "Now try the full system to retrieve matches to an unseen query image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query image\n",
    "query = imread(data_dir+'/data/queries/mistery-building1.jpg') / 255.\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "ax.imshow(query)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load precomputed data \n",
    "# SIFT feature points and descriptors for the query (mistery building)\n",
    "data = np.load(data_dir+\"/data_part3/mistery-building1_sift_ellipse_kps_descs.npy\", encoding='latin1', allow_pickle=True)\n",
    "\n",
    "kpsq = data.item().get('keypoints')\n",
    "descrsq = data.item().get('descriptors')\n",
    "\n",
    "# imdb index\n",
    "data = np.load(data_dir+\"/data_part3/imdb.npy\", encoding='latin1', allow_pickle=True)\n",
    "\n",
    "index = data.item().get('index') # all sift feature histograms of the database images\n",
    "vocab = data.item().get('vocabulary') # visual word vocabulary (ellipse)\n",
    "idf = data.item().get('idf') # inverse document frequency \n",
    "imdbImDir = str.encode(data_dir+'/') + data.item().get('imdir') # imgfile information\n",
    "imdbImNames = data.item().get('imnames')\n",
    "imdbImWords = data.item().get('imwords') # quantized sift descriptors\n",
    "imdbImKps = data.item().get('imkps') # keypoint frames of database images\n",
    "\n",
    "# histogram descriptor of the query image\n",
    "h = getHistogramFromDescriptor(vocab, idf, descrsq)\n",
    "\n",
    "# Score the other images by similarity to the query\n",
    "tic = time.clock()\n",
    "scores = index.dot(h)\n",
    "time_index = time.clock() - tic\n",
    "scores = scores.squeeze()\n",
    "\n",
    "# Rescore the top 25 images based on the number of inlier matches.\n",
    "sorted_scores = np.sort(scores, axis=0)\n",
    "id_sorted_scores = np.argsort(scores, axis=0)\n",
    "sorted_scores = sorted_scores[::-1]\n",
    "id_sorted_scores = id_sorted_scores[::-1]\n",
    "\n",
    "words = kdtreequery(vocab, descrsq, maxNumChecks=1024)\n",
    "\n",
    "for rank in range(25):\n",
    "    ii = id_sorted_scores[rank]\n",
    "    db_instance_words = imdbImWords[ii]\n",
    "    matches = matchWords(words, db_instance_words)\n",
    "    instance_kps = imdbImKps[ii].T\n",
    "    inliers_word, H_word = geometricVerification(kpsq, instance_kps, matches, 3)\n",
    "    newScore = len(inliers_word)\n",
    "    scores[id_sorted_scores[rank]] = newScore\n",
    "\n",
    "# Plot results by decreasing score\n",
    "plotRetrievedImages(imdbImDir, imdbImNames, scores, 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
