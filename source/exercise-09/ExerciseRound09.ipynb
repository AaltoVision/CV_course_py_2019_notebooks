{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "#   Exercise09 notebook.\n",
    "#\n",
    "# Copyright (C) 2018 Santiago Cortes, Juha Ylioinas\n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later); please refer to the file \n",
    "# Licence.txt, included with the software, for details.\n",
    "\n",
    "# Preparations\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import from_data_file, theta_to_model, model_to_theta, initial_model, logistic, \\\n",
    "    log_sum_exp_over_rows, classification_performance\n",
    "\n",
    "# Select data directory\n",
    "\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "elif os.path.isdir('../data'):\n",
    "    course_data_dir = '../data'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-09-data')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 9\n",
    "\n",
    "The problems should be solved before the exercise session and solutions returned via\n",
    "MyCourses. Upload the files: (1) a pdf file containing your written answers to Exercise\n",
    "1, and (2) both the pdf and .ipynb versions of the notebook containing your answers\n",
    "to Exercises 2 and 3. Scanned, **neatly** handwritten, solutions are ok for problem 1. If\n",
    "possible, combine your pdf solution of Exercise 1 with the notebook pdf into a single pdf\n",
    "and return that with the notebook .ipynb file.\n",
    "\n",
    "Notice also that the last two problems\n",
    "can be done without solving Exercise 1 since the solutions are already written out in the\n",
    "subtasks of Exercise 1 (i.e. only the derivations are missing and asked in Exercise 1).\n",
    "\n",
    "If you have not studied basics of neural networks in previous courses and the problem\n",
    "context of these exercises is not clear, it may be helpful to check the slides of the first four\n",
    "lectures of prof. Hinton’s course “Introduction to neural networks and machine learning”:  \n",
    "http://www.cs.toronto.edu/~hinton/coursera_slides.html    \n",
    "http://www.cs.toronto.edu/~hinton/coursera_lectures.html (lecture videos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Neural  networks  and  backpropagation\n",
    "This is a pen-&-paper problem. See Exercise09penandpaper.pdf for the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Image  classification  using  a  neural  network\n",
    "The first exercise problem above gives the solution to Part 2 of the second programming\n",
    "assignment of professor Hinton’s course “Introduction to neural networks and machine\n",
    "learning”. The assignment and related material are available at\n",
    "https://www.cs.toronto.edu/~tijmen/csc321/assignments/a2/.\n",
    "\n",
    "\n",
    "Check out the contents of the above web page and complete\n",
    "the programming task of Part 2 according to the instructions given there. The code template for the python version is below. The solution for the\n",
    "pen and paper part of the task is already given above in __Exercise 1__. Hence, the programming part is\n",
    "a relatively straightforward implementation and can be done without carrying out the\n",
    "derivations since the results of the derivations are already given in __Exercise 1__ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradient(model, data, wd_coefficient):\n",
    "    import sys\n",
    "    base_theta = model_to_theta(model)\n",
    "    h = 1e-2\n",
    "    correctness_threshold = 1e-5\n",
    "    analytic_gradient_struct = d_loss_by_d_model(model, data, wd_coefficient)\n",
    "\n",
    "    analytic_gradient = model_to_theta(analytic_gradient_struct);\n",
    "    if True in np.isnan(analytic_gradient) or True in np.isinf(analytic_gradient):\n",
    "        sys.exit('Your gradient computation produced a NaN or infinity. That is an error.')\n",
    "    # We want to test the gradient not for every element of theta, because that's a \n",
    "    # lot of work. Instead, we test for only a few elements. If there's an error, this \n",
    "    # is probably enough to find that error.\n",
    "    # We want to first test the hid_to_class gradient, because that's most likely \n",
    "    # to be correct (it's the easier one).\n",
    "    # Let's build a list of theta indices to check. We'll check 20 elements of \n",
    "    # hid_to_class, and 80 elements of input_to_hid (it's bigger than hid_to_class).\n",
    "    input_to_hid_theta_size = model['input_to_hid'].size\n",
    "    hid_to_class_theta_size = model['hid_to_class'].size\n",
    "    big_prime = 1299721; # 1299721 is prime and thus ensures a somewhat random-like selection of indices.\n",
    "    hid_to_class_indices_to_check = np.mod(big_prime * np.arange(20), hid_to_class_theta_size) \\\n",
    "                                        + input_to_hid_theta_size\n",
    "    input_to_hid_indices_to_check = np.mod(big_prime * np.arange(80), input_to_hid_theta_size)\n",
    "    a = hid_to_class_indices_to_check[np.newaxis,:]\n",
    "    b = input_to_hid_indices_to_check[np.newaxis,:]\n",
    "    indices_to_check = np.ravel(np.hstack((a,b)))\n",
    "\n",
    "    for i in range(100):\n",
    "        test_index = indices_to_check[i]\n",
    "        analytic_here = analytic_gradient[test_index]\n",
    "        theta_step = base_theta * 0\n",
    "        theta_step[test_index] = h\n",
    "        contribution_distances = np.array([-4.,  -3.,  -2.,  -1.,   1.,   2.,   3.,   4.])\n",
    "        contribution_weights = np.array([1/280., -4/105., 1/5., -4/5., 4/5., -1/5., 4/105., -1/280.])\n",
    "        temp = 0;\n",
    "        for contribution_index in range(8):\n",
    "            temp = temp + loss(theta_to_model(base_theta + theta_step * \\\n",
    "                                              contribution_distances[contribution_index]), data, wd_coefficient) * \\\n",
    "                                                contribution_weights[contribution_index]\n",
    "        fd_here = temp / h\n",
    "        diff = np.abs(analytic_here - fd_here)\n",
    "        \n",
    "        if True in (diff > correctness_threshold) and \\\n",
    "            True in (diff / (np.abs(analytic_here) + np.abs(fd_here)) > correctness_threshold):\n",
    "            part_names = ['input_to_hid', 'hid_to_class']\n",
    "            sys.exit('Theta element #{} (part of {}), with value {}, has finite difference gradient {} but analytic gradient {}. That looks like an error.\\n'.format(test_index, part_names[(i<=20)], base_theta[test_index], fd_here, analytic_here))\n",
    "        if i==20: \n",
    "            print('Gradient test passed for hid_to_class. ')\n",
    "        if i==100: \n",
    "            print('Gradient test passed for input_to_hid. ')\n",
    "    print('Gradient test passed. That means that the gradient that your code computed is within 0.001%% of the gradient that the finite difference approximation computed, so the gradient calculation procedure is probably correct (not certainly, but probably).\\n')\n",
    "    \n",
    "def forward_pass(model, data):\n",
    "    # This function does the forward pass through the network: calculating the states of all units, and some related data. \n",
    "    # This function is used in functions loss() and d_loss_by_d_model().  \n",
    "  \n",
    "    # model.input_to_hid is a matrix of size <number of hidden units> by <number of inputs i.e. 256>. It contains the weights from the input units to the hidden units.\n",
    "    # model.hid_to_class is a matrix of size <number of classes i.e. 10> by <number of hidden units>. It contains the weights from the hidden units to the softmax units.\n",
    "    # data.inputs is a matrix of size <number of inputs i.e. 256> by <number of data cases>. Each column describes a different data case. \n",
    "    # data.targets is a matrix of size <number of classes i.e. 10> by <number of data cases>. Each column describes a different data case. It contains a one-of-N encoding of the class, i.e. one element in every column is 1 and the others are 0.\n",
    "    \n",
    "    hid_input = np.dot(model['input_to_hid'], data['inputs']) # input to the hidden units, i.e. before the logistic. size: <number of hidden units> by <number of data cases>\n",
    "    hid_output = logistic(hid_input) # output of the hidden units, i.e. after the logistic. size: <number of hidden units> by <number of data cases>\n",
    "    class_input = np.dot(model['hid_to_class'], hid_output) # input to the components of the softmax. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "  \n",
    "    # The following three lines of code implement the softmax.\n",
    "    # However, it's written differently from what the lectures say.\n",
    "    # In the lectures, a softmax is described using an exponential divided by a sum of exponentials.\n",
    "    # What we do here is exactly equivalent (you can check the math or just check it in practice), but this is more numerically stable. \n",
    "    # \"Numerically stable\" means that this way, there will never be really big numbers involved.\n",
    "    # The exponential in the lectures can lead to really big numbers, which are fine in mathematical equations, but can lead to all sorts of problems in Matlab\n",
    "    # Matlab isn't well prepared to deal with really large numbers, like the number 10 to the power 1000. Computations with such numbers get unstable, so we avoid them.\n",
    "\n",
    "    class_normalizer = log_sum_exp_over_rows(class_input) # log(sum(exp of class_input)) is what we subtract to get properly normalized log class probabilities. size: <1> by <number of data cases>\n",
    "    log_class_prob = class_input - np.tile(class_normalizer, (class_input.shape[0], 1)) # log of probability of each class. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "    class_prob = np.exp(log_class_prob) # probability of each class. Each column (i.e. each case) sums to 1. size: <number of classes, i.e. 10> by <number of data cases>\n",
    "\n",
    "    return hid_input, hid_output, class_input, log_class_prob, class_prob\n",
    "\n",
    "def loss(model, data, wd_coefficient):\n",
    "    hid_input, hid_output, class_input, log_class_prob, class_prob = forward_pass(model, data);\n",
    "    classification_loss = -np.mean(np.sum(np.multiply(log_class_prob, data['target']), 0)) # select the right log class probability using that sum; then take the mean over all data cases.\n",
    "    wd_loss = (np.sum(np.ravel(model['input_to_hid']) ** 2 ) + np.sum(np.ravel(model['hid_to_class']) ** 2 )) / 2. * wd_coefficient; # weight decay loss. very straightforward: E = 1/2 * wd_coeffecient * parameters^2\n",
    "    ret = classification_loss + wd_loss\n",
    "    return ret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_loss_by_d_model(model, data, wd_coefficient):\n",
    "    # model.input_to_hid is a matrix of size <number of hidden units> by <number of inputs i.e. 256>\n",
    "    # model.hid_to_class is a matrix of size <number of classes i.e. 10> by <number of hidden units>\n",
    "    # data.inputs is a matrix of size <number of inputs i.e. 256> by <number of data cases>\n",
    "    # data.targets is a matrix of size <number of classes i.e. 10> by <number of data cases>\n",
    "\n",
    "    # The returned object <ret> is supposed to be exactly like parameter <model>, i.e. it has fields ret.input_to_hid and ret.hid_to_class, and those are of the same shape as they are in <model>.\n",
    "    # However, in <ret>, the contents of those matrices are gradients (d loss by d weight), instead of weights.\n",
    "    ret = dict()\n",
    "    # This is the only function that you're expected to change. Right now, it just returns a lot of zeros, which is obviously not the correct output. Your job is to change that.\n",
    "    #--your-code-starts-here--#\n",
    "    ret['input_to_hid'] = model['input_to_hid'] * 0;\n",
    "    ret['hid_to_class'] = model['hid_to_class'] * 0;\n",
    "    #--your-code-ends-here--#\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2(wd_coefficient, n_hid, n_iters, learning_rate, momentum_multiplier, do_early_stopping, mini_batch_size):\n",
    "    model = initial_model(n_hid)\n",
    "    datas = from_data_file(data_dir)\n",
    "\n",
    "    n_training_cases = datas['train']['inputs'].shape[1]\n",
    "    if n_iters != 0:\n",
    "        print(\"Now testing the gradient on the whole training set...\")\n",
    "        test_gradient(model, datas['train'], wd_coefficient)\n",
    "    \n",
    "    # optimization\n",
    "    training_batch = dict()\n",
    "    best_so_far = dict()\n",
    "    theta = model_to_theta(model)\n",
    "    momentum_speed = theta * 0.\n",
    "    training_data_losses = []\n",
    "    validation_data_losses = []\n",
    "    if do_early_stopping:\n",
    "        best_so_far['theta'] = -1 # this will be overwritten soon\n",
    "        best_so_far['validation_loss'] = np.Inf\n",
    "        best_so_far['after_n_iters'] = -1\n",
    "        \n",
    "    for optimization_iteration_i in range(1, n_iters+1):\n",
    "        model = theta_to_model(theta)\n",
    "        training_batch_start = np.mod((optimization_iteration_i-1) * mini_batch_size, n_training_cases);  \n",
    "        training_batch['inputs'] = datas['train']['inputs'][:, training_batch_start : training_batch_start + mini_batch_size]\n",
    "        training_batch['target'] = datas['train']['target'][:, training_batch_start : training_batch_start + mini_batch_size]\n",
    "        gradient = model_to_theta(d_loss_by_d_model(model, training_batch, wd_coefficient))\n",
    "        momentum_speed = np.multiply(momentum_speed, momentum_multiplier) - gradient;\n",
    "        theta = theta + momentum_speed * learning_rate;\n",
    "        model = theta_to_model(theta);\n",
    "        training_data_losses.append(loss(model, datas['train'], wd_coefficient))\n",
    "        validation_data_losses.append(loss(model, datas['val'], wd_coefficient))\n",
    "        \n",
    "        if do_early_stopping and validation_data_losses[-1] < best_so_far['validation_loss']:\n",
    "            best_so_far['theta'] = theta; # this will be overwritten soon\n",
    "            best_so_far['validation_loss'] = validation_data_losses[-1]\n",
    "            best_so_far['after_n_iters'] = optimization_iteration_i\n",
    "            \n",
    "        if np.mod(optimization_iteration_i, np.round(n_iters / 10.)) == 0:\n",
    "            print('After {} optimization iterations, training data loss is {}, and validation data loss is {}\\n'.format(optimization_iteration_i, training_data_losses[-1], validation_data_losses[-1]))\n",
    "    \n",
    "        if optimization_iteration_i == n_iters: # check gradient again, this time with more typical parameters and with a different data size\n",
    "            print('Now testing the gradient on just a mini-batch instead of the whole training set... ')\n",
    "            test_gradient(model, training_batch, wd_coefficient)\n",
    "            \n",
    "    if do_early_stopping:\n",
    "        print('Early stopping: validation loss was lowest after {} iterations. We chose the model that we had then.\\n'.format(best_so_far['after_n_iters']))\n",
    "        theta = best_so_far['theta']\n",
    "    \n",
    "    # the optimization is finished. Now do some reporting.\n",
    "    model = theta_to_model(theta)\n",
    "    if n_iters != 0:\n",
    "        ax = plt.figure(1, figsize=(15,10))\n",
    "        plt.plot(training_data_losses, 'b')\n",
    "        plt.plot(validation_data_losses, 'r')\n",
    "        plt.tick_params(labelsize=25)\n",
    "        ax.legend(('training', 'validation'), fontsize=25)\n",
    "        plt.ylabel('loss', fontsize=25);\n",
    "        plt.xlabel('iteration number', fontsize=25);\n",
    "        plt.show()\n",
    "    \n",
    "    datas2 = [datas['train'], datas['val'], datas['test']]\n",
    "    data_names = ['training', 'validation', 'test'];\n",
    "    for data_i in range(3):\n",
    "        data = datas2[data_i]\n",
    "        data_name = data_names[data_i]\n",
    "        print('\\nThe total loss on the {} data is {}\\n'.format(data_name, loss(model, data, wd_coefficient)))\n",
    "        print('The classification loss (i.e. without weight decay) on the {} data is {}\\n'.format(data_name, loss(model, data, 0)));\n",
    "        print('The classification error rate on the {} data is {}\\n'.format(data_name, classification_performance(model, data)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the neural network\n",
    "#a2(0, 10, 70, 20.0, 0, False, 4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have completed the programming part, run a2(0, 10, 30, 0.01, 0, False, 10) and report the resulting training data classification loss here:\n",
    "\n",
    "Training data classification loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Optimisation using backpropagation\n",
    "Do Part 3 of the assignment as described at\n",
    "http://www.cs.toronto.edu/~tijmen/csc321/assignments/a2/\n",
    "\n",
    "The task is to experiment with the example code given above and report your findings.\n",
    "There is no need to program anything in this part but completing it requires that Part 2\n",
    "is successfully solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with learning_rate = 0.002, 0.01, 0.05, 0.2, 1.0, 5.0, and 20.0, and with and without momentum_multiplier=0.9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
