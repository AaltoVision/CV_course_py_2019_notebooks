{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    "if (code_show){\n",
    "$('div.input').hide();\n",
    "} else {\n",
    "$('div.input').show();\n",
    "}\n",
    "code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:\n",
    "#   Exercise10 notebook.\n",
    "#\n",
    "# Copyright (C) 2019 Antti Parviainen\n",
    "# Based on the SSD PyTorch implementation by Max deGroot and Ellis Brown \n",
    "#\n",
    "# This software is distributed under the GNU General Public \n",
    "# Licence (version 2 or later);\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from ssd import build_ssd\n",
    "from data import VOCDetection, VOCAnnotationTransform\n",
    "from data import VOC_CLASSES as labels\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Select data directory\n",
    "if os.path.isdir('/coursedata'):\n",
    "    course_data_dir = '/coursedata'\n",
    "    docker = False\n",
    "elif os.path.isdir('../../coursedata'):\n",
    "    docker = True\n",
    "    course_data_dir = '../../coursedata'\n",
    "else:\n",
    "    # Specify course_data_dir on your machine\n",
    "    course_data_dir = '/home/jovyan/work/coursedata/'\n",
    "\n",
    "print('The data directory is %s' % course_data_dir)\n",
    "data_dir = os.path.join(course_data_dir, 'exercise-10-data')\n",
    "print('Data stored in %s' % data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-E4850 Computer Vision Exercise Round 10\n",
    "The problems should be solved before the exercise session and solutions returned via\n",
    "MyCourses. Upload to MyCourses both: this Jupyter Notebook (.ipynb) file containing your solutions and the exported pdf version of this Notebook file. If there are both programming and pen & paper tasks kindly combine the two pdf files (your scanned/LaTeX solutions and the exported Notebook) into a single pdf and submit that with the Notebook (.ipynb) file. <br><br> Note that you should be sure that everything that you need to implement works with the pictures specified in this exercise round.\n",
    "\n",
    "**Docker users:**  \n",
    "Unfortunately PyTorch was not included in the Docker image I created for this years course and therefore this assignment will not work with your docker container based on that image. For this exercise the patented parts of opencv are not needed so it's still easy to work on the exercise locally by creating a Python environment and installing the correct packages.  \n",
    "\n",
    "For example using Conda:  \n",
    "conda create --name cv2019  \n",
    "conda activate cv2019  \n",
    "conda install -c pytorch pytorch    \n",
    "conda install -c conda-forge opencv, jupyterlab, matplotlib  \n",
    "jupyter lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Object detection with SSD: Single Shot MultiBox Object Detector, in PyTorch\n",
    "\n",
    "The goal of this task is to learn the basics of deep learning based object detection with SSD by experimenting with the provided code and by reading the original [Single Shot MultiBox Detector](http://arxiv.org/abs/1512.02325) publication by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg from 2016.\n",
    "\n",
    "Read the research paper linked above and experiment with the provided sample code according to the instructions below. Then answer the questions a), b), c) below and return your answers. Note that scientific publications are written for domain experts and some details may be challenging to understand if necessary background information is missing. However, don’t worry if you don’t understand all details. You should be able to grasp the overall idea and answer the questions even if some details would be difficult to understand.\n",
    "\n",
    "The code implements the following steps to demonstrate SSD:\n",
    "\n",
    "1. Build the SSD300 architecture and load pretrained weights on the VOC07 trainval dataset\n",
    "2. Load 4 random sample images from the VOC07 dataset\n",
    "3. Preprocess the images to the correct input form and show the preprocessed images\n",
    "4. Run the sample images through the SSD network, parse the detections and show the results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **a) In the beginning of the publication the authors argue about the relevance and novelty of their work. Summarise their main arguments and the evidence they present to support their arguments.**\n",
    "\n",
    "Type your answer here:\n",
    "\n",
    "#### **b) SSD consists of two networks: a \"truncated base network\" and a network of added \"convolutional feature layers to the end of the truncated base network.\" What is the purpose of the base network? Which base network do the authors of the SSD publication use, and what dataset was used to train the base network?**\n",
    "\n",
    "Type your answer here:\n",
    "\n",
    "#### **c) SSD has it’s own loss function, defined in chapter 2.2 in the original publication. What are the two attributes this loss function observes? How are these defined (short explanation without any formulas is sufficient) and how do they help the network to minimise the object detection error?**\n",
    "\n",
    "Type your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the SSD architecture and the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_ssd('test', 300, 21)    # initialize SSD\n",
    "net.load_weights(data_dir+'/weights/ssd300_mAP_77.43_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Sample Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(4):\n",
    "    if docker:\n",
    "        # if local storage use a 300 image subset of the test set\n",
    "        img_id = np.random.randint(1,high = 300)\n",
    "        image = cv2.imread(data_dir+'/voc_images/'+f\"{img_id:06d}\"'.jpg', cv2.IMREAD_COLOR)\n",
    "    else:\n",
    "        # if JupyterLab use the full test set\n",
    "        # here we specify year (07 or 12) and dataset ('test', 'val', 'train') \n",
    "        testset = VOCDetection(data_dir+'/VOCdevkit', [('2007', 'val')], None, VOCAnnotationTransform())\n",
    "        img_id = np.random.randint(0,high = len(testset))\n",
    "        image = testset.pull_image(img_id)\n",
    "\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    images.append(rgb_image)\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "ax = axes.ravel()\n",
    "ax[0].imshow(images[0])\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(images[1])\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(images[2])\n",
    "ax[2].axis('off')\n",
    "ax[3].imshow(images[3])\n",
    "ax[3].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Randomly sampled images from the dataset\", fontsize=20)\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Pre-process the input images\n",
    "Using the torchvision package, we can apply multiple built-in transorms. At test time we resize our image to 300x300, subtract the dataset's mean rgb values, and swap the color channels for input to SSD300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(images):\n",
    "    preprocessed_images = []\n",
    "    for image in images:\n",
    "        x = cv2.resize(image, (300, 300)).astype(np.float32)\n",
    "        x -= (104.0, 117.0, 123.0)\n",
    "        x = x.astype(np.float32)\n",
    "        preprocessed_images.append(x)\n",
    "    return preprocessed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = preprocess_inputs(images)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "ax = axes.ravel()\n",
    "ax[0].imshow(preprocessed_images[0])\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(preprocessed_images[1])\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(preprocessed_images[2])\n",
    "ax[2].axis('off')\n",
    "ax[3].imshow(preprocessed_images[3])\n",
    "ax[3].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Preprocessed input images\", fontsize=20)\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run SSD on the sample images and show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(images, nrows, ncols, figsize = (10,10), threshold = 0.6, title=True):\n",
    "    \n",
    "    if nrows * ncols != len(images):\n",
    "        print(\"Subgrid dimensions don't match with the number of images.\")\n",
    "        return\n",
    "    \n",
    "    preprocessed_images = preprocess_inputs(images)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    \n",
    "    if len(images) != 1:\n",
    "        ax = axes.ravel()\n",
    "    else:\n",
    "        ax = [axes]\n",
    "    \n",
    "    for it, input_image in enumerate(images):    \n",
    "        # Process data for the network\n",
    "        # swap color channels\n",
    "        x = preprocessed_images[it][:, :, ::-1].copy()\n",
    "        # change the order\n",
    "        x = torch.from_numpy(x).permute(2, 0, 1)\n",
    "        # wrap the image in a Variable so it is recognized by PyTorch autograd\n",
    "        xx = Variable(x.unsqueeze(0))\n",
    "        if torch.cuda.is_available():\n",
    "            xx = xx.cuda()\n",
    "            \n",
    "        # SSD Forward Pass\n",
    "        y = net(xx)\n",
    "        detections = y.data\n",
    "\n",
    "        # colormap for the bounding boxes\n",
    "        colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "\n",
    "        # scale each detection back up to the image\n",
    "        scale = torch.Tensor(input_image.shape[1::-1]).repeat(2)\n",
    "        for i in range(detections.size(1)):\n",
    "            j = 0\n",
    "            while detections[0,i,j,0] >= threshold:\n",
    "                score = detections[0,i,j,0]\n",
    "                label_name = labels[i-1]\n",
    "                display_txt = '%s: %.2f'%(label_name, score)\n",
    "                pt = (detections[0,i,j,1:]*scale).cpu().numpy()\n",
    "                coords = (pt[0], pt[1]), pt[2]-pt[0]+1, pt[3]-pt[1]+1\n",
    "                color = colors[i]\n",
    "                ax[it].add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "                ax[it].text(pt[0], pt[1], display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "                j+=1\n",
    "        ax[it].imshow(images[it])\n",
    "        ax[it].axis('off')\n",
    "    plt.tight_layout()\n",
    "    if title:\n",
    "        plt.suptitle(\"Detection results at threshold: %1.2f\" %threshold, fontsize=20)\n",
    "        plt.subplots_adjust(top=0.95)\n",
    "    plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outputs with confidence scores lower than a threshold. The default threshold is 60%.\n",
    "run_network(images, nrows=2, ncols=2, figsize=(10,10), threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Evaluating the SSD network on some more challenging input images\n",
    "\n",
    "To better detect challenging inputs the authors have implemented data augmentation described in chapter 2.2 and 3.2 of the publication and in [reference 14](https://arxiv.org/abs/1312.5402). Read the chapters in the publication and selectively read the main points of reference 14 and answer the following questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1**\n",
    "#### **a) Based on the results (test images1) below what kind of objects are easier for the network to detect?**\n",
    "\n",
    "Type your answer here:\n",
    "\n",
    "#### **b)  Would switching from input size 300x300 to 512x512 make the problem better, worse, or have no impact? Elaborate on why or why not. Would designing a object detector that uses HD resolution of 1920x1080, or even higher, images as inputs be a good idea? Elaborate on why or why not.**\n",
    "\n",
    "Type your answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photo credit: to burningman.org. Used under the fair use principles for transformative educational purposes.\n",
    "image11 = cv2.imread(data_dir+'/test_images/img11.jpg', cv2.IMREAD_COLOR)\n",
    "images1=[cv2.cvtColor(image11, cv2.COLOR_BGR2RGB)]\n",
    "run_network(images1, nrows=1, ncols=1, figsize=(10,10), threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2**\n",
    "#### **a) Based on the results (test images2) below elaborate why the detector has trouble detecting the objects?**\n",
    "\n",
    "Type your answer here:\n",
    "\n",
    "#### **b) Have the authors of the SSD publication tried to mitigate this problem? If yes, briefly explain how.**\n",
    "\n",
    "Type your answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photo credit: free wallpaper image. Used under the fair use principles for transformative educational purposes.\n",
    "image21 = cv2.imread(data_dir+'/test_images/img21.jpg', cv2.IMREAD_COLOR)\n",
    "# Photo credit: David Dodman, KNOM. Used under the fair use principles for transformative educational purposes.\n",
    "image22 = cv2.imread(data_dir+'/test_images/img22.jpg', cv2.IMREAD_COLOR)\n",
    "images2=[cv2.cvtColor(image21, cv2.COLOR_BGR2RGB),cv2.cvtColor(image22, cv2.COLOR_BGR2RGB)]\n",
    "run_network(images2, nrows=2, ncols=1, figsize=(25,25), threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3**\n",
    "#### **a) Based on the results (test images3) below, elaborate why the detector has trouble detecting the objects?**\n",
    "\n",
    "Type your answer here:\n",
    "\n",
    "#### **b) Have the authors of the SSD publication tried to mitigate this problem? If yes, briefly explain how.**\n",
    "\n",
    "Type your answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photo credit: Duncan Rawlinson - Duncan.co photo from flickr. Creative Commons license. https://creativecommons.org/licenses/by-nc/2.0/\n",
    "image31 = cv2.imread(data_dir+'/test_images/img31.jpg', cv2.IMREAD_COLOR)\n",
    "# Photo credit: Karri Huhtanen from flickr. Creative Commons license. https://creativecommons.org/licenses/by-nc/2.0/\n",
    "image32 = cv2.imread(data_dir+'/test_images/img32.jpg', cv2.IMREAD_COLOR)\n",
    "images3=[cv2.cvtColor(image31, cv2.COLOR_BGR2RGB),cv2.cvtColor(image32, cv2.COLOR_BGR2RGB)]\n",
    "run_network(images3, nrows=2, ncols=1, figsize=(15,15), threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4**\n",
    "#### **a) Based on the results (test images4) below elaborate why the detector has trouble detecting objects in the first/upper image, but is able to detect objects in the second/lower image which contains the left part of the upper image? Note: you can double click the upper image to show it in actual size.**\n",
    "\n",
    "Type your answer here:\n",
    "\n",
    "#### **b) Have the authors of the SSD publication tried to mitigate this problem? If yes, briefly explain how.**\n",
    "\n",
    "Type your answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photo credit: Brad Templeton. Used under the fair use principles for transformative educational purposes.\n",
    "image41 = cv2.imread(data_dir+'/test_images/img41.jpg', cv2.IMREAD_COLOR)\n",
    "image42 = cv2.imread(data_dir+'/test_images/img42.jpg', cv2.IMREAD_COLOR)\n",
    "images4 = [cv2.cvtColor(image41, cv2.COLOR_BGR2RGB), cv2.cvtColor(image42, cv2.COLOR_BGR2RGB)]\n",
    "# run the first image without title information to show it properly\n",
    "run_network([images4[0]], nrows=1, ncols=1, figsize=(100,100), threshold=0.6, title=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_network([images4[1]], nrows=1, ncols=1, figsize=(10,10), threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5**\n",
    "#### **a) One of the test images in (images5) was perhaps accidentally flipped vertically and as can be seen the detector has trouble detecting objects if there's a significant rotation. Have the authors of the SSD publication tried to mitigate this problem? If yes, briefly explain how. If no, propose a naive method to mitigate it and explain why it usually might not be necessary or a good idea(more harm than good).**\n",
    "\n",
    "Type your answer here:\n",
    "\n",
    "#### **b) Is a convolutional network naturally, without specifically adressing the issue, able to detect objects if there are small rotations? If yes, briefly explain which part of the network helps to mitigate the effects of rotation and why. (Hint: what layers are sometimes between two convolutional layers)**\n",
    "\n",
    "Type your answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photo credit: Susan Schmitz / shutterstock. Used under the fair use principles for transformative educational purposes.\n",
    "image51 = cv2.imread(data_dir+'/test_images/img51.jpg', cv2.IMREAD_COLOR)\n",
    "image52 = cv2.imread(data_dir+'/test_images/img52.jpg', cv2.IMREAD_COLOR)\n",
    "images5=[cv2.cvtColor(image51, cv2.COLOR_BGR2RGB),cv2.cvtColor(image52, cv2.COLOR_BGR2RGB)]\n",
    "run_network(images5, nrows=2, ncols=1, figsize=(15,15), threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.6**\n",
    "\n",
    "#### **a) Incrementally lower the detection confidence threshold, run SSD on the test images and observe the results. What are the upsides and downsides of lowering the confidence threshold? How could you measure the effect of the confidence threshold? Assume you have some object detection task that you want to apply the detector to. What kind of object detection tasks could benefit from a lower confidence threshold and what kind of tasks need a high confidence threshold?**\n",
    "\n",
    "Type your answer here\n",
    "\n",
    "#### **b) Watch the following YouTube video of a detector that is similar to SSD called [YOLO V2 (You Only Look Once)](https://www.youtube.com/watch?v=VOC3huqHrss) and use the knowledge from previous tasks to answer the following questions: In what object detection tasks is a state of the art object detector especially useful and able to perform better than a human? In what object detection tasks is a human better than a state of the art object detector? Give examples of both.**\n",
    "\n",
    "Type your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
